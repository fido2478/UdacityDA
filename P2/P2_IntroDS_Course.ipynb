{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "7.0\n",
      "\n",
      "[ 44.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "The following code is to help you play with Numpy, which is a library \n",
    "that provides functions that are especially useful when you have to\n",
    "work with large arrays and matrices of numeric data, like doing \n",
    "matrix matrix multiplications. Also, Numpy is battle tested and \n",
    "optimized so that it runs fast, much faster than if you were working\n",
    "with Python lists directly.\n",
    "'''\n",
    "\n",
    "'''\n",
    "The array object class is the foundation of Numpy, and Numpy arrays are like\n",
    "lists in Python, except that every thing inside an array must be of the\n",
    "same type, like int or float.\n",
    "'''\n",
    "# Change False to True to see Numpy arrays in action\n",
    "if False:\n",
    "    array = np.array([1, 4, 5, 8], float)\n",
    "    print array\n",
    "    print \"\"\n",
    "    array = np.array([[1, 2, 3], [4, 5, 6]], float)  # a 2D array/Matrix\n",
    "    print array\n",
    "\n",
    "'''\n",
    "You can index, slice, and manipulate a Numpy array much like you would with a\n",
    "a Python list.\n",
    "'''\n",
    "# Change False to True to see array indexing and slicing in action\n",
    "if False:\n",
    "    array = np.array([1, 4, 5, 8], float)\n",
    "    print array\n",
    "    print \"\"\n",
    "    print array[1]\n",
    "    print \"\"\n",
    "    print array[:2]\n",
    "    print \"\"\n",
    "    array[1] = 5.0\n",
    "    print array[1]\n",
    "\n",
    "# Change False to True to see Matrix indexing and slicing in action\n",
    "if False:\n",
    "    two_D_array = np.array([[1, 2, 3], [4, 5, 6]], float)\n",
    "    print two_D_array\n",
    "    print \"\"\n",
    "    print two_D_array[1][1]\n",
    "    print \"\"\n",
    "    print two_D_array[1, :]\n",
    "    print \"\"\n",
    "    print two_D_array[:, 2]\n",
    "\n",
    "'''\n",
    "Here are some arithmetic operations that you can do with Numpy arrays\n",
    "'''\n",
    "# Change False to True to see Array arithmetics in action\n",
    "if False:\n",
    "    array_1 = np.array([1, 2, 3], float)\n",
    "    array_2 = np.array([5, 2, 6], float)\n",
    "    print array_1 + array_2\n",
    "    print \"\"\n",
    "    print array_1 - array_2\n",
    "    print \"\"\n",
    "    print array_1 * array_2\n",
    "\n",
    "# Change False to True to see Matrix arithmetics in action\n",
    "if False:\n",
    "    array_1 = np.array([[1, 2], [3, 4]], float)\n",
    "    array_2 = np.array([[5, 6], [7, 8]], float)\n",
    "    print array_1 + array_2\n",
    "    print \"\"\n",
    "    print array_1 - array_2\n",
    "    print \"\"\n",
    "    print array_1 * array_2\n",
    "\n",
    "'''\n",
    "In addition to the standard arthimetic operations, Numpy also has a range of\n",
    "other mathematical operations that you can apply to Numpy arrays, such as\n",
    "mean and dot product.\n",
    "\n",
    "Both of these functions will be useful in later programming quizzes.\n",
    "'''\n",
    "if True:\n",
    "    array_1 = np.array([1, 2, 3], float)\n",
    "    array_2 = np.array([[6], [7], [8]], float)\n",
    "    print np.mean(array_1)\n",
    "    print np.mean(array_2)\n",
    "    print \"\"\n",
    "    print np.dot(array_1, array_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cockroach    False\n",
      "Fish         False\n",
      "Mini Pig     False\n",
      "Puppy         True\n",
      "Kitten        True\n",
      "dtype: bool\n",
      "\n",
      "Puppy     4\n",
      "Kitten    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "The following code is to help you play with the concept of Series in Pandas.\n",
    "\n",
    "You can think of Series as an one-dimensional object that is similar to\n",
    "an array, list, or column in a database. By default, it will assign an\n",
    "index label to each item in the Series ranging from 0 to N, where N is\n",
    "the number of items in the Series minus one.\n",
    "\n",
    "Please feel free to play around with the concept of Series and see what it does\n",
    "\n",
    "*This playground is inspired by Greg Reda's post on Intro to Pandas Data Structures:\n",
    "http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/\n",
    "'''\n",
    "# Change False to True to create a Series object\n",
    "if False:\n",
    "    series = pd.Series(['Dave', 'Cheng-Han', 'Udacity', 42, -1789710578])\n",
    "    print series\n",
    "\n",
    "'''\n",
    "You can also manually assign indices to the items in the Series when\n",
    "creating the series\n",
    "'''\n",
    "\n",
    "# Change False to True to see custom index in action\n",
    "if False:\n",
    "    series = pd.Series(['Dave', 'Cheng-Han', 359, 9001],\n",
    "                       index=['Instructor', 'Curriculum Manager',\n",
    "                              'Course Number', 'Power Level'])\n",
    "    print series\n",
    "\n",
    "'''\n",
    "You can use index to select specific items from the Series\n",
    "'''\n",
    "# Change False to True to see Series indexing in action\n",
    "if False:\n",
    "    series = pd.Series(['Dave', 'Cheng-Han', 359, 9001],\n",
    "                       index=['Instructor', 'Curriculum Manager',\n",
    "                              'Course Number', 'Power Level'])\n",
    "    print series['Instructor']\n",
    "    print \"\"\n",
    "    print series[['Instructor', 'Curriculum Manager', 'Course Number']]\n",
    "\n",
    "'''\n",
    "You can also use boolean operators to select specific items from the Series\n",
    "'''\n",
    "# Change False to True to see boolean indexing in action\n",
    "if True:\n",
    "    cuteness = pd.Series([1, 2, 3, 4, 5], index=['Cockroach', 'Fish', 'Mini Pig',\n",
    "                                                 'Puppy', 'Kitten'])\n",
    "    print cuteness > 3\n",
    "    print \"\"\n",
    "    print cuteness[cuteness > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses     int64\n",
      "team      object\n",
      "wins       int64\n",
      "year       int64\n",
      "dtype: object\n",
      "\n",
      "          losses       wins         year\n",
      "count   8.000000   8.000000     8.000000\n",
      "mean    6.625000   9.375000  2011.125000\n",
      "std     3.377975   3.377975     0.834523\n",
      "min     1.000000   4.000000  2010.000000\n",
      "25%     5.000000   7.500000  2010.750000\n",
      "50%     6.000000  10.000000  2011.000000\n",
      "75%     8.500000  11.000000  2012.000000\n",
      "max    12.000000  15.000000  2012.000000\n",
      "\n",
      "   losses     team  wins  year\n",
      "0       5    Bears    11  2010\n",
      "1       8    Bears     8  2011\n",
      "2       6    Bears    10  2012\n",
      "3       1  Packers    15  2011\n",
      "4       5  Packers    11  2012\n",
      "\n",
      "   losses     team  wins  year\n",
      "3       1  Packers    15  2011\n",
      "4       5  Packers    11  2012\n",
      "5      10    Lions     6  2010\n",
      "6       6    Lions    10  2011\n",
      "7      12    Lions     4  2012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "The following code is to help you play with the concept of Dataframe in Pandas.\n",
    "\n",
    "You can think of a Dataframe as something with rows and columns. It is\n",
    "similar to a spreadsheet, a database table, or R's data.frame object.\n",
    "\n",
    "*This playground is inspired by Greg Reda's post on Intro to Pandas Data Structures:\n",
    "http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/\n",
    "'''\n",
    "\n",
    "'''\n",
    "To create a dataframe, you can pass a dictionary of lists to the Dataframe\n",
    "constructor:\n",
    "1) The key of the dictionary will be the column name\n",
    "2) The associating list will be the values within that column.\n",
    "'''\n",
    "# Change False to True to see Dataframes in action\n",
    "if False:\n",
    "    data = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012],\n",
    "            'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions',\n",
    "                     'Lions', 'Lions'],\n",
    "            'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n",
    "            'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n",
    "    football = pd.DataFrame(data)\n",
    "    print football\n",
    "\n",
    "'''\n",
    "Pandas also has various functions that will help you understand some basic\n",
    "information about your data frame. Some of these functions are:\n",
    "1) dtypes: to get the datatype for each column\n",
    "2) describe: useful for seeing basic statistics of the dataframe's numerical\n",
    "   columns\n",
    "3) head: displays the first five rows of the dataset\n",
    "4) tail: displays the last five rows of the dataset\n",
    "'''\n",
    "# Change False to True to see these functions in action\n",
    "if True:\n",
    "    data = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012],\n",
    "            'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions',\n",
    "                     'Lions', 'Lions'],\n",
    "            'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n",
    "            'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n",
    "    football = pd.DataFrame(data)\n",
    "    print football.dtypes\n",
    "    print \"\"\n",
    "    print football.describe()\n",
    "    print \"\"\n",
    "    print football.head()\n",
    "    print \"\"\n",
    "    print football.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bronze       countries  gold  sliver\n",
      "0        9    Russian Fed.    13      11\n",
      "1       10          Norway    11       5\n",
      "2        5          Canada    10      10\n",
      "3       12   United States     9       7\n",
      "4        9     Netherlands     8       7\n",
      "5        5         Germany     8       6\n",
      "6        2     Switzerland     6       3\n",
      "7        1         Belarus     5       0\n",
      "8        5         Austria     4       8\n",
      "9        7          France     4       4\n",
      "10       1          Poland     4       1\n",
      "11       2           China     3       4\n",
      "12       2           Korea     3       3\n",
      "13       6          Sweden     2       7\n",
      "14       2  Czech Republic     2       4\n",
      "15       4        Slovenia     2       2\n",
      "16       3           Japan     1       4\n",
      "17       1         Finland     1       3\n",
      "18       2   Great Britain     1       1\n",
      "19       1         Ukraine     1       0\n",
      "20       0        Slovakia     1       0\n",
      "21       6           Italy     0       2\n",
      "22       2          Latvia     0       2\n",
      "23       1       Australia     0       2\n",
      "24       0         Croatia     0       1\n",
      "25       1      Kazakhstan     0       0\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame, Series\n",
    "\n",
    "#################\n",
    "# Syntax Reminder:\n",
    "#\n",
    "# The following code would create a two-column pandas DataFrame\n",
    "# named df with columns labeled 'name' and 'age':\n",
    "#\n",
    "# people = ['Sarah', 'Mike', 'Chrisna']\n",
    "# ages  =  [28, 32, 25]\n",
    "# df = DataFrame({'name' : Series(people),\n",
    "#                 'age'  : Series(ages)})\n",
    "\n",
    "def create_dataframe():\n",
    "    '''\n",
    "    Create a pandas dataframe called 'olympic_medal_counts_df' containing\n",
    "    the data from the table of 2014 Sochi winter olympics medal counts.  \n",
    "\n",
    "    The columns for this dataframe should be called \n",
    "    'country_name', 'gold', 'silver', and 'bronze'.  \n",
    "\n",
    "    There is no need to  specify row indexes for this dataframe \n",
    "    (in this case, the rows will automatically be assigned numbered indexes).\n",
    "    \n",
    "    You do not need to call the function in your code when running it in the\n",
    "    browser - the grader will do that automatically when you submit or test it.\n",
    "    '''\n",
    "\n",
    "    countries = ['Russian Fed.', 'Norway', 'Canada', 'United States',\n",
    "                 'Netherlands', 'Germany', 'Switzerland', 'Belarus',\n",
    "                 'Austria', 'France', 'Poland', 'China', 'Korea', \n",
    "                 'Sweden', 'Czech Republic', 'Slovenia', 'Japan',\n",
    "                 'Finland', 'Great Britain', 'Ukraine', 'Slovakia',\n",
    "                 'Italy', 'Latvia', 'Australia', 'Croatia', 'Kazakhstan']\n",
    "\n",
    "    gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "    silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0]\n",
    "    bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1]\n",
    "\n",
    "    # your code here\n",
    "    data={'countries':Series(countries),\n",
    "          'gold':Series(gold),\n",
    "          'sliver':Series(silver),\n",
    "          'bronze':Series(bronze),\n",
    "          }\n",
    "    olympic_medal_counts_df=DataFrame(data)\n",
    "\n",
    "    return olympic_medal_counts_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print create_dataframe()\n",
    "#cd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   losses   team  wins  year\n",
      "0       5  Bears    11  2010\n",
      "\n",
      "   losses   team  wins  year\n",
      "0       5  Bears    11  2010\n",
      "\n",
      "   losses     team  wins  year\n",
      "3       1  Packers    15  2011\n",
      "4       5  Packers    11  2012\n",
      "\n",
      "   losses     team  wins  year\n",
      "0       5    Bears    11  2010\n",
      "3       1  Packers    15  2011\n",
      "4       5  Packers    11  2012\n",
      "\n",
      "   losses     team  wins  year\n",
      "3       1  Packers    15  2011\n",
      "4       5  Packers    11  2012\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "You can think of a DataFrame as a group of Series that share an index.\n",
    "This makes it easy to select specific columns that you want from the \n",
    "DataFrame. \n",
    "\n",
    "Also a couple pointers:\n",
    "1) Selecting a single column from the DataFrame will return a Series\n",
    "2) Selecting multiple columns from the DataFrame will return a DataFrame\n",
    "\n",
    "*This playground is inspired by Greg Reda's post on Intro to Pandas Data Structures:\n",
    "http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/\n",
    "'''\n",
    "# Change False to True to see Series indexing in action\n",
    "if False:\n",
    "    data = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012],\n",
    "            'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions',\n",
    "                     'Lions', 'Lions'],\n",
    "            'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n",
    "            'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n",
    "    football = pd.DataFrame(data)\n",
    "    print football['year']\n",
    "    print ''\n",
    "    print football.year  # shorthand for football['year']\n",
    "    print ''\n",
    "    print football[['year', 'wins', 'losses']]\n",
    "\n",
    "'''\n",
    "Row selection can be done through multiple ways.\n",
    "\n",
    "Some of the basic and common methods are:\n",
    "   1) Slicing\n",
    "   2) An individual index (through the functions iloc or loc)\n",
    "   3) Boolean indexing\n",
    "\n",
    "You can also combine multiple selection requirements through boolean\n",
    "operators like & (and) or | (or)\n",
    "'''\n",
    "# Change False to True to see boolean indexing in action\n",
    "if True:\n",
    "    data = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012],\n",
    "            'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions',\n",
    "                     'Lions', 'Lions'],\n",
    "            'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n",
    "            'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n",
    "    football = pd.DataFrame(data)\n",
    "    print football.iloc[[0]]\n",
    "    print \"\"\n",
    "    print football.loc[[0]]\n",
    "    print \"\"\n",
    "    print football[3:5]\n",
    "    print \"\"\n",
    "    print football[football.wins > 10]\n",
    "    print \"\"\n",
    "    print football[(football.wins > 10) & (football.team == \"Packers\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2380952381\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame, Series\n",
    "import numpy\n",
    "\n",
    "\n",
    "def avg_medal_count():\n",
    "    '''\n",
    "    Compute the average number of bronze medals earned by countries who \n",
    "    earned at least one gold medal.  \n",
    "    \n",
    "    Save this to a variable named avg_bronze_at_least_one_gold. You do not\n",
    "    need to call the function in your code when running it in the browser -\n",
    "    the grader will do that automatically when you submit or test it.\n",
    "    \n",
    "    HINT-1:\n",
    "    You can retrieve all of the values of a Pandas column from a \n",
    "    data frame, \"df\", as follows:\n",
    "    df['column_name']\n",
    "    \n",
    "    HINT-2:\n",
    "    The numpy.mean function can accept as an argument a single\n",
    "    Pandas column. \n",
    "    \n",
    "    For example, numpy.mean(df[\"col_name\"]) would return the \n",
    "    mean of the values located in \"col_name\" of a dataframe df.\n",
    "    '''\n",
    "\n",
    "\n",
    "    countries = ['Russian Fed.', 'Norway', 'Canada', 'United States',\n",
    "                 'Netherlands', 'Germany', 'Switzerland', 'Belarus',\n",
    "                 'Austria', 'France', 'Poland', 'China', 'Korea', \n",
    "                 'Sweden', 'Czech Republic', 'Slovenia', 'Japan',\n",
    "                 'Finland', 'Great Britain', 'Ukraine', 'Slovakia',\n",
    "                 'Italy', 'Latvia', 'Australia', 'Croatia', 'Kazakhstan']\n",
    "\n",
    "    gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "    silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0]\n",
    "    bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1]\n",
    "    \n",
    "    olympic_medal_counts = {'country_name':countries,\n",
    "                            'gold': gold,\n",
    "                            'silver': silver,\n",
    "                            'bronze': bronze}\n",
    "    df = DataFrame(olympic_medal_counts)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    avg_bronze_at_least_one_gold=numpy.mean(df[\"bronze\"][df[\"gold\"]>=1])\n",
    "\n",
    "    return avg_bronze_at_least_one_gold\n",
    "if __name__ == \"__main__\":\n",
    "    print avg_medal_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze    3.807692\n",
      "gold      3.807692\n",
      "silver    3.730769\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "\n",
    "def avg_medal_count():\n",
    "    '''\n",
    "    Using the dataframe's apply method, create a new Series called \n",
    "    avg_medal_count that indicates the average number of gold, silver,\n",
    "    and bronze medals earned amongst countries who earned at \n",
    "    least one medal of any kind at the 2014 Sochi olympics.  Note that\n",
    "    the countries list already only includes countries that have earned\n",
    "    at least one medal. No additional filtering is necessary.\n",
    "    \n",
    "    You do not need to call the function in your code when running it in the\n",
    "    browser - the grader will do that automatically when you submit or test it.\n",
    "    '''\n",
    "\n",
    "    countries = ['Russian Fed.', 'Norway', 'Canada', 'United States',\n",
    "                 'Netherlands', 'Germany', 'Switzerland', 'Belarus',\n",
    "                 'Austria', 'France', 'Poland', 'China', 'Korea', \n",
    "                 'Sweden', 'Czech Republic', 'Slovenia', 'Japan',\n",
    "                 'Finland', 'Great Britain', 'Ukraine', 'Slovakia',\n",
    "                 'Italy', 'Latvia', 'Australia', 'Croatia', 'Kazakhstan']\n",
    "\n",
    "    gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "    silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0]\n",
    "    bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1]\n",
    "    \n",
    "    olympic_medal_counts = {'country_name':countries,\n",
    "                            'gold': gold,\n",
    "                            'silver': silver,\n",
    "                            'bronze': bronze}    \n",
    "    df = DataFrame(olympic_medal_counts)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    avg_medal_count=numpy.mean(df[:])\n",
    "    return avg_medal_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print avg_medal_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country_name  points\n",
      "0     Russian Fed.      83\n",
      "1           Norway      64\n",
      "2           Canada      65\n",
      "3    United States      62\n",
      "4      Netherlands      55\n",
      "5          Germany      49\n",
      "6      Switzerland      32\n",
      "7          Belarus      21\n",
      "8          Austria      37\n",
      "9           France      31\n",
      "10          Poland      19\n",
      "11           China      22\n",
      "12           Korea      20\n",
      "13          Sweden      28\n",
      "14  Czech Republic      18\n",
      "15        Slovenia      16\n",
      "16           Japan      15\n",
      "17         Finland      11\n",
      "18   Great Britain       8\n",
      "19         Ukraine       5\n",
      "20        Slovakia       4\n",
      "21           Italy      10\n",
      "22          Latvia       6\n",
      "23       Australia       5\n",
      "24         Croatia       2\n",
      "25      Kazakhstan       1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "\n",
    "def numpy_dot():\n",
    "    '''\n",
    "    Imagine a point system in which each country is awarded 4 points for each\n",
    "    gold medal,  2 points for each silver medal, and one point for each \n",
    "    bronze medal.  \n",
    "\n",
    "    Using the numpy.dot function, create a new dataframe called \n",
    "    'olympic_points_df' that includes:\n",
    "        a) a column called 'country_name' with the country name\n",
    "        b) a column called 'points' with the total number of points the country\n",
    "           earned at the Sochi olympics.\n",
    "           \n",
    "    You do not need to call the function in your code when running it in the\n",
    "    browser - the grader will do that automatically when you submit or test it.\n",
    "    '''\n",
    "\n",
    "    countries = ['Russian Fed.', 'Norway', 'Canada', 'United States',\n",
    "                 'Netherlands', 'Germany', 'Switzerland', 'Belarus',\n",
    "                 'Austria', 'France', 'Poland', 'China', 'Korea', \n",
    "                 'Sweden', 'Czech Republic', 'Slovenia', 'Japan',\n",
    "                 'Finland', 'Great Britain', 'Ukraine', 'Slovakia',\n",
    "                 'Italy', 'Latvia', 'Australia', 'Croatia', 'Kazakhstan']\n",
    "\n",
    "    gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "    silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0]\n",
    "    bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1]\n",
    " \n",
    "    # YOUR CODE HERE\n",
    "    olympic_points_df=DataFrame({'country_name':countries,'points':numpy.dot([4,2,1],[gold,silver,bronze])})    \n",
    "    return olympic_points_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print numpy_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786756453423\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def simple_heuristic(file_path):\n",
    "    '''\n",
    "    In this exercise, we will perform some rudimentary practices similar to those of\n",
    "    an actual data scientist.\n",
    "    \n",
    "    Part of a data scientist's job is to use her or his intuition and insight to\n",
    "    write algorithms and heuristics. A data scientist also creates mathematical models \n",
    "    to make predictions based on some attributes from the data that they are examining.\n",
    "\n",
    "    We would like for you to take your knowledge and intuition about the Titanic\n",
    "    and its passengers' attributes to predict whether or not the passengers survived\n",
    "    or perished. You can read more about the Titanic and specifics about this dataset at:\n",
    "    http://en.wikipedia.org/wiki/RMS_Titanic\n",
    "    http://www.kaggle.com/c/titanic-gettingStarted\n",
    "        \n",
    "    In this exercise and the following ones, you are given a list of Titantic passengers\n",
    "    and their associated information. More information about the data can be seen at the \n",
    "    link below:\n",
    "    http://www.kaggle.com/c/titanic-gettingStarted/data. \n",
    "\n",
    "    For this exercise, you need to write a simple heuristic that will use\n",
    "    the passengers' gender to predict if that person survived the Titanic disaster.\n",
    "    \n",
    "    You prediction should be 78% accurate or higher.\n",
    "        \n",
    "    Here's a simple heuristic to start off:\n",
    "       1) If the passenger is female, your heuristic should assume that the\n",
    "       passenger survived.\n",
    "       2) If the passenger is male, you heuristic should\n",
    "       assume that the passenger did not survive.\n",
    "    \n",
    "    You can access the gender of a passenger via passenger['Sex'].\n",
    "    If the passenger is male, passenger['Sex'] will return a string \"male\".\n",
    "    If the passenger is female, passenger['Sex'] will return a string \"female\".\n",
    "\n",
    "    Write your prediction back into the \"predictions\" dictionary. The\n",
    "    key of the dictionary should be the passenger's id (which can be accessed\n",
    "    via passenger[\"PassengerId\"]) and the associated value should be 1 if the\n",
    "    passenger survied or 0 otherwise.\n",
    "\n",
    "    For example, if a passenger is predicted to have survived:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 1\n",
    "\n",
    "    And if a passenger is predicted to have perished in the disaster:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 0\n",
    "    \n",
    "    You can also look at the Titantic data that you will be working with\n",
    "    at the link below:\n",
    "    https://www.dropbox.com/s/r5f9aos8p9ri9sa/titanic_data.csv\n",
    "    '''\n",
    "\n",
    "    predictions = {}\n",
    "    real = []\n",
    "    df = pandas.read_csv(file_path)\n",
    "    for passenger_index, passenger in df.iterrows():\n",
    "        passenger_id = passenger['PassengerId']\n",
    "      \n",
    "        # Your code here:\n",
    "        # For example, let's assume that if the passenger\n",
    "        # is a male, then the passenger survived.\n",
    "        if passenger['Sex'] == 'male':\n",
    "            predictions[passenger_id] = 0\n",
    "        else:\n",
    "            predictions[passenger_id] = 1\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def check_accuracy(file_name):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    df = pandas.read_csv(file_name)\n",
    "    predictions = simple_heuristic(file_name)\n",
    "    for row_index, row in df.iterrows():\n",
    "        total_count += 1\n",
    "        if predictions[row['PassengerId']] == row['Survived']:\n",
    "            correct_count += 1\n",
    "    return float(correct_count)/float(total_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_heuristic_success_rate = check_accuracy('titanic_data.csv')\n",
    "    print simple_heuristic_success_rate\n",
    "    \n",
    "#Your heuristic is 78.68% accurate. Is it 78% or better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791245791246\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def complex_heuristic(file_path):\n",
    "    '''\n",
    "    You are given a list of Titantic passengers and their associated\n",
    "    information. More information about the data can be seen at the link below:\n",
    "    http://www.kaggle.com/c/titanic-gettingStarted/data\n",
    "\n",
    "    For this exercise, you need to write a more sophisticated algorithm\n",
    "    that will use the passengers' gender and their socioeconomical class and age \n",
    "    to predict if they survived the Titanic diaster. \n",
    "    \n",
    "    You prediction should be 79% accurate or higher.\n",
    "    \n",
    "    Here's the algorithm, predict the passenger survived if:\n",
    "    1) If the passenger is female or\n",
    "    2) if his/her socioeconomic status is high AND if the passenger is under 18\n",
    "    \n",
    "    Otherwise, your algorithm should predict that the passenger perished in the disaster.\n",
    "    \n",
    "    Or more specifically in terms of coding:\n",
    "    female or (high status and under 18)\n",
    "    \n",
    "    You can access the gender of a passenger via passenger['Sex'].\n",
    "    If the passenger is male, passenger['Sex'] will return a string \"male\".\n",
    "    If the passenger is female, passenger['Sex'] will return a string \"female\".\n",
    "    \n",
    "    You can access the socioeconomic status of a passenger via passenger['Pclass']:\n",
    "    High socioeconomic status -- passenger['Pclass'] is 1\n",
    "    Medium socioeconomic status -- passenger['Pclass'] is 2\n",
    "    Low socioeconomic status -- passenger['Pclass'] is 3\n",
    "\n",
    "    You can access the age of a passenger via passenger['Age'].\n",
    "    \n",
    "    Write your prediction back into the \"predictions\" dictionary. The\n",
    "    key of the dictionary should be the Passenger's id (which can be accessed\n",
    "    via passenger[\"PassengerId\"]) and the associated value should be 1 if the\n",
    "    passenger survived or 0 otherwise. \n",
    "\n",
    "    For example, if a passenger is predicted to have survived:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 1\n",
    "\n",
    "    And if a passenger is predicted to have perished in the disaster:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 0\n",
    "    \n",
    "    You can also look at the Titantic data that you will be working with\n",
    "    at the link below:\n",
    "    https://www.dropbox.com/s/r5f9aos8p9ri9sa/titanic_data.csv\n",
    "    '''\n",
    "\n",
    "    predictions = {}\n",
    "    real = []\n",
    "    df = pandas.read_csv(file_path)\n",
    "    for passenger_index, passenger in df.iterrows():\n",
    "        passenger_id = passenger['PassengerId']\n",
    "        # \n",
    "        # your code here\n",
    "        # for example, assuming that passengers who are male\n",
    "        # and older than 18 surived:\n",
    "        if passenger['Sex'] == 'female' or (passenger['Age'] < 18 and passenger['Pclass'] < 2):\n",
    "            predictions[passenger_id] = 1\n",
    "        else:\n",
    "            predictions[passenger_id] = 0\n",
    "    return predictions\n",
    "\n",
    "def check_accuracy(file_name):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    df = pandas.read_csv(file_name)\n",
    "    predictions = complex_heuristic(file_name)\n",
    "    for row_index, row in df.iterrows():\n",
    "        total_count += 1\n",
    "        if predictions[row['PassengerId']] == row['Survived']:\n",
    "            correct_count += 1\n",
    "    return float(correct_count)/float(total_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    complex_heuristic_success_rate = check_accuracy('titanic_data.csv')\n",
    "    print complex_heuristic_success_rate\n",
    "\n",
    "#Your heuristic is 79.12% accurate. Is it 79% or better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 891\n",
      "0.800224466891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def custom_heuristic(file_path):\n",
    "    '''\n",
    "    You are given a list of Titantic passengers and their associated\n",
    "    information. More information about the data can be seen at the link below:\n",
    "    http://www.kaggle.com/c/titanic-gettingStarted/data\n",
    "\n",
    "    For this exercise, you need to write a custom heuristic that will take\n",
    "    in some combination of the passenger's attributes and predict if the passenger\n",
    "    survived the Titanic diaster.\n",
    "\n",
    "    Can your custom heuristic beat 80% accuracy?\n",
    "    \n",
    "    The available attributes are:\n",
    "    Pclass          Passenger Class\n",
    "                    (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "    Name            Name\n",
    "    Sex             Sex\n",
    "    Age             Age\n",
    "    SibSp           Number of Siblings/Spouses Aboard\n",
    "    Parch           Number of Parents/Children Aboard\n",
    "    Ticket          Ticket Number\n",
    "    Fare            Passenger Fare\n",
    "    Cabin           Cabin\n",
    "    Embarked        Port of Embarkation\n",
    "                    (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "                    \n",
    "    SPECIAL NOTES:\n",
    "    Pclass is a proxy for socioeconomic status (SES)\n",
    "    1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "    Age is in years; fractional if age less than one\n",
    "    If the age is estimated, it is in the form xx.5\n",
    "\n",
    "    With respect to the family relation variables (i.e. SibSp and Parch)\n",
    "    some relations were ignored. The following are the definitions used\n",
    "    for SibSp and Parch.\n",
    "\n",
    "    Sibling:  brother, sister, stepbrother, or stepsister of passenger aboard Titanic\n",
    "    Spouse:   husband or wife of passenger aboard Titanic (mistresses and fiancees ignored)\n",
    "    Parent:   mother or father of passenger aboard Titanic\n",
    "    Child:    son, daughter, stepson, or stepdaughter of passenger aboard Titanic\n",
    "    \n",
    "    Write your prediction back into the \"predictions\" dictionary. The\n",
    "    key of the dictionary should be the passenger's id (which can be accessed\n",
    "    via passenger[\"PassengerId\"]) and the associating value should be 1 if the\n",
    "    passenger survvied or 0 otherwise. \n",
    "\n",
    "    For example, if a passenger is predicted to have survived:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 1\n",
    "\n",
    "    And if a passenger is predicted to have perished in the disaster:\n",
    "    passenger_id = passenger['PassengerId']\n",
    "    predictions[passenger_id] = 0\n",
    "    \n",
    "    You can also look at the Titantic data that you will be working with\n",
    "    at the link below:\n",
    "    https://www.dropbox.com/s/r5f9aos8p9ri9sa/titanic_data.csv\n",
    "    '''\n",
    "\n",
    "    predictions = {}\n",
    "    real=[]\n",
    "    df = pandas.read_csv(file_path)\n",
    "    for passenger_index, passenger in df.iterrows():\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        passenger_id = passenger['PassengerId']\n",
    "        if passenger['Sex']=='female' or (passenger['Age'] < 15 and passenger['Pclass'] < 3) :\n",
    "            predictions[passenger_id]=1\n",
    "        else:\n",
    "            predictions[passenger_id]=0\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def check_accuracy(file_name):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    df = pandas.read_csv(file_name)\n",
    "    predictions = custom_heuristic(file_name)\n",
    "    for row_index, row in df.iterrows():\n",
    "        total_count += 1\n",
    "        if predictions[row['PassengerId']] == row['Survived']:\n",
    "            correct_count += 1\n",
    "    #print correct_count, total_count\n",
    "    return float(correct_count)/float(total_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    custom_heuristic_success_rate = check_accuracy('titanic_data.csv')\n",
    "    print custom_heuristic_success_rate\n",
    "\n",
    "\n",
    "#Your heuristic is 80.02% accurate. Is it 80% or better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             David Aardsma\n",
      "1                Hank Aaron\n",
      "2              Tommie Aaron\n",
      "3                  Don Aase\n",
      "4                 Andy Abad\n",
      "5             Fernando Abad\n",
      "6               John Abadie\n",
      "7            Ed Abbaticchio\n",
      "8                Bert Abbey\n",
      "9             Charlie Abbey\n",
      "10               Dan Abbott\n",
      "11              Fred Abbott\n",
      "12             Glenn Abbott\n",
      "13              Jeff Abbott\n",
      "14               Jim Abbott\n",
      "15              Kurt Abbott\n",
      "16              Kyle Abbott\n",
      "17               Ody Abbott\n",
      "18              Paul Abbott\n",
      "19                  Al Aber\n",
      "20        Frank Abercrombie\n",
      "21       Reggie Abercrombie\n",
      "22          Bill Abernathie\n",
      "23          Brent Abernathy\n",
      "24            Ted Abernathy\n",
      "25            Ted Abernathy\n",
      "26          Woody Abernathy\n",
      "27            Cliff Aberson\n",
      "28              Harry Ables\n",
      "29              Shawn Abner\n",
      "                ...        \n",
      "18559      Jordan Zimmerman\n",
      "18560     Jordan Zimmermann\n",
      "18561         Roy Zimmerman\n",
      "18562        Ryan Zimmerman\n",
      "18563          Charlie Zink\n",
      "18564           Walter Zink\n",
      "18565            Frank Zinn\n",
      "18566              Guy Zinn\n",
      "18567            Jimmy Zinn\n",
      "18568           Bill Zinser\n",
      "18569           Alan Zinter\n",
      "18570            Bud Zipfel\n",
      "18571           Richie Zisk\n",
      "18572            Barry Zito\n",
      "18573        Billy Zitzmann\n",
      "18574              Ed Zmich\n",
      "18575           Ben Zobrist\n",
      "18576      Peter Zoccolillo\n",
      "18577            Sam Zoldak\n",
      "18578           Eddie Zosky\n",
      "18579            Bill Zuber\n",
      "18580             Jon Zuber\n",
      "18581          Julio Zuleta\n",
      "18582           Joel Zumaya\n",
      "18583           Mike Zunino\n",
      "18584            Bob Zupcic\n",
      "18585            Frank Zupo\n",
      "18586          Paul Zuvella\n",
      "18587       George Zuverink\n",
      "18588        Dutch Zwilling\n",
      "Name: nameFull, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "def add_full_name(path_to_csv, path_to_new_csv):\n",
    "    #Assume you will be reading in a csv file with the same columns that the\n",
    "    #Lahman baseball data set has -- most importantly, there are columns\n",
    "    #called 'nameFirst' and 'nameLast'.\n",
    "    #1) Write a function that reads a csv\n",
    "    #located at \"path_to_csv\" into a pandas dataframe and adds a new column\n",
    "    #called 'nameFull' with a player's full name.\n",
    "    #\n",
    "    #For example:\n",
    "    #   for Hank Aaron, nameFull would be 'Hank Aaron', \n",
    "\t#\n",
    "\t#2) Write the data in the pandas dataFrame to a new csv file located at\n",
    "\t#path_to_new_csv\n",
    "\n",
    "    #WRITE YOUR CODE HERE\n",
    "    pf = pandas.read_csv(path_to_csv)\n",
    "    pf['nameFull']=pf['nameFirst']+' '+pf['nameLast']\n",
    "    pf.to_csv(path_to_new_csv)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # For local use only\n",
    "    # If you are running this on your own machine add the path to the\n",
    "    # Lahman baseball csv and a path for the new csv.\n",
    "    # The dataset can be downloaded from this website: http://www.seanlahman.com/baseball-archive/statistics\n",
    "    # We are using the file Master.csv\n",
    "    path_to_csv = \"Master.csv\"\n",
    "    path_to_new_csv = \"Master_new.csv\"\n",
    "    add_full_name(path_to_csv, path_to_new_csv)\n",
    "    \n",
    "    pf = pandas.read_csv(path_to_new_csv)\n",
    "    print pf.nameFull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         registrar             enrolment_agency\n",
      "0   Allahabad Bank            Tera Software Ltd\n",
      "1   Allahabad Bank            Tera Software Ltd\n",
      "2   Allahabad Bank  Vakrangee Softwares Limited\n",
      "3   Allahabad Bank  Vakrangee Softwares Limited\n",
      "4   Allahabad Bank  Vakrangee Softwares Limited\n",
      "5   Allahabad Bank  Vakrangee Softwares Limited\n",
      "6   Allahabad Bank  Vakrangee Softwares Limited\n",
      "7   Allahabad Bank  Vakrangee Softwares Limited\n",
      "8   Allahabad Bank  Vakrangee Softwares Limited\n",
      "9   Allahabad Bank  Vakrangee Softwares Limited\n",
      "10  Allahabad Bank  Vakrangee Softwares Limited\n",
      "11  Allahabad Bank  Vakrangee Softwares Limited\n",
      "12  Allahabad Bank  Vakrangee Softwares Limited\n",
      "13  Allahabad Bank  Vakrangee Softwares Limited\n",
      "14  Allahabad Bank  Vakrangee Softwares Limited\n",
      "15  Allahabad Bank  Vakrangee Softwares Limited\n",
      "16  Allahabad Bank  Vakrangee Softwares Limited\n",
      "17  Allahabad Bank  Vakrangee Softwares Limited\n",
      "18  Allahabad Bank  Vakrangee Softwares Limited\n",
      "19  Allahabad Bank  Vakrangee Softwares Limited\n",
      "20  Allahabad Bank  Vakrangee Softwares Limited\n",
      "21  Allahabad Bank  Vakrangee Softwares Limited\n",
      "22  Allahabad Bank  Vakrangee Softwares Limited\n",
      "23  Allahabad Bank  Vakrangee Softwares Limited\n",
      "24  Allahabad Bank  Vakrangee Softwares Limited\n",
      "25  Allahabad Bank  Vakrangee Softwares Limited\n",
      "26  Allahabad Bank  Vakrangee Softwares Limited\n",
      "27  Allahabad Bank  Vakrangee Softwares Limited\n",
      "28  Allahabad Bank  Vakrangee Softwares Limited\n",
      "29  Allahabad Bank  Vakrangee Softwares Limited\n",
      "30  Allahabad Bank  Vakrangee Softwares Limited\n",
      "31  Allahabad Bank  Vakrangee Softwares Limited\n",
      "32  Allahabad Bank  Vakrangee Softwares Limited\n",
      "33  Allahabad Bank  Vakrangee Softwares Limited\n",
      "34  Allahabad Bank  Vakrangee Softwares Limited\n",
      "35  Allahabad Bank  Vakrangee Softwares Limited\n",
      "36  Allahabad Bank  Vakrangee Softwares Limited\n",
      "37  Allahabad Bank  Vakrangee Softwares Limited\n",
      "38  Allahabad Bank  Vakrangee Softwares Limited\n",
      "39  Allahabad Bank  Vakrangee Softwares Limited\n",
      "40  Allahabad Bank  Vakrangee Softwares Limited\n",
      "41  Allahabad Bank  Vakrangee Softwares Limited\n",
      "42  Allahabad Bank  Vakrangee Softwares Limited\n",
      "43  Allahabad Bank  Vakrangee Softwares Limited\n",
      "44  Allahabad Bank  Vakrangee Softwares Limited\n",
      "45  Allahabad Bank  Vakrangee Softwares Limited\n",
      "46  Allahabad Bank  Vakrangee Softwares Limited\n",
      "47  Allahabad Bank  Vakrangee Softwares Limited\n",
      "48  Allahabad Bank  Vakrangee Softwares Limited\n",
      "49  Allahabad Bank  Vakrangee Softwares Limited\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "def select_first_50(filename):\n",
    "    # Read in our aadhaar_data csv to a pandas dataframe.  Afterwards, we rename the columns\n",
    "    # by replacing spaces with underscores and setting all characters to lowercase, so the\n",
    "    # column names more closely resemble columns names one might find in a table.\n",
    "    aadhaar_data = pandas.read_csv(filename)\n",
    "    aadhaar_data.rename(columns = lambda x: x.replace(' ', '_').lower(), inplace=True)\n",
    "\n",
    "    # Select out the first 50 values for \"registrar\" and \"enrolment_agency\"\n",
    "    # in the aadhaar_data table using SQL syntax. \n",
    "    #\n",
    "    # Note that \"enrolment_agency\" is spelled with one l. Also, the order\n",
    "    # of the select does matter. Make sure you select registrar then enrolment agency\n",
    "    # in your query.\n",
    "    #\n",
    "    # You can download a copy of the aadhaar data that we are passing \n",
    "    # into this exercise below:\n",
    "    # https://www.dropbox.com/s/vn8t4uulbsfmalo/aadhaar_data.csv\n",
    "    q = \"\"\"\n",
    "    -- YOUR QUERY HERE\n",
    "    select registrar, enrolment_agency from aadhaar_data limit 50\n",
    "    \"\"\"\n",
    "    \n",
    "    #Execute your SQL command against the pandas frame\n",
    "    aadhaar_solution = pandasql.sqldf(q.lower(), locals())\n",
    "    return aadhaar_solution  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print select_first_50(\"aadhaar_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gender           district  sum(aadhaar_generated)\n",
      "0        F         Ahmadnagar                      45\n",
      "1        F        Ahmed Nagar                       0\n",
      "2        F          Ahmedabad                       1\n",
      "3        F              Ajmer                      27\n",
      "4        F              Akola                       5\n",
      "5        F          Alirajpur                      71\n",
      "6        F          Allahabad                      15\n",
      "7        F              Alwar                      14\n",
      "8        F             Ambala                       7\n",
      "9        F           Amravati                       0\n",
      "10       F           Amritsar                      30\n",
      "11       F            Anuppur                     101\n",
      "12       F        Ashok Nagar                       1\n",
      "13       F         Aurangabad                      19\n",
      "14       F           Balaghat                     287\n",
      "15       F          Bangalore                     433\n",
      "16       F    Bangalore Rural                       9\n",
      "17       F              Banka                       0\n",
      "18       F           Banswara                      28\n",
      "19       F              Baran                       5\n",
      "20       F            Barwani                      34\n",
      "21       F           Bathinda                      43\n",
      "22       F               Beed                      25\n",
      "23       F            Belgaum                      59\n",
      "24       F            Bellary                      65\n",
      "25       F              Betul                      41\n",
      "26       F          Bhagalpur                       0\n",
      "27       F           Bhandara                       4\n",
      "28       F          Bharatpur                      94\n",
      "29       F           Bhilwara                      50\n",
      "..     ...                ...                     ...\n",
      "498      M              Sirsa                     158\n",
      "499      M            Solapur                      80\n",
      "500      M          Sonbhadra                       7\n",
      "501      M            Sonipat                      14\n",
      "502      M        South Delhi                       5\n",
      "503      M      South Tripura                       0\n",
      "504      M   South West Delhi                       4\n",
      "505      M  Sri Muktsar Sahib                      21\n",
      "506      M         Tarn Taran                     125\n",
      "507      M              Thane                     144\n",
      "508      M          Tikamgarh                     265\n",
      "509      M     Tiruvannamalai                       1\n",
      "510      M               Tonk                      49\n",
      "511      M             Tumkur                       1\n",
      "512      M            Udaipur                      16\n",
      "513      M             Ujjain                      66\n",
      "514      M             Umaria                     137\n",
      "515      M                Una                       3\n",
      "516      M             Valsad                       2\n",
      "517      M            Vellore                       0\n",
      "518      M            Vidisha                      84\n",
      "519      M         Viluppuram                      18\n",
      "520      M             Wardha                       1\n",
      "521      M             Washim                      33\n",
      "522      M         West Delhi                       3\n",
      "523      M     West Singhbhum                     227\n",
      "524      M       West Tripura                      24\n",
      "525      M             Yadgir                      12\n",
      "526      M       Yamuna Nagar                     149\n",
      "527      M           Yavatmal                      54\n",
      "\n",
      "[528 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "def aggregate_query(filename):\n",
    "    # Read in our aadhaar_data csv to a pandas dataframe.  Afterwards, we rename the columns\n",
    "    # by replacing spaces with underscores and setting all characters to lowercase, so the\n",
    "    # column names more closely resemble columns names one might find in a table.\n",
    "    \n",
    "    aadhaar_data = pandas.read_csv(filename)\n",
    "    aadhaar_data.rename(columns = lambda x: x.replace(' ', '_').lower(), inplace=True)\n",
    "\n",
    "    # Write a query that will select from the aadhaar_data table how many men and how \n",
    "    # many women over the age of 50 have had aadhaar generated for them in each district.\n",
    "    # aadhaar_generated is a column in the Aadhaar Data that denotes the number who have had\n",
    "    # aadhaar generated in each row of the table.\n",
    "    #\n",
    "    # Note that in this quiz, the SQL query keywords are case sensitive. \n",
    "    # For example, if you want to do a sum make sure you type 'sum' rather than 'SUM'.\n",
    "    #\n",
    "\n",
    "    # The possible columns to select from aadhaar data are:\n",
    "    #     1) registrar\n",
    "    #     2) enrolment_agency\n",
    "    #     3) state\n",
    "    #     4) district\n",
    "    #     5) sub_district\n",
    "    #     6) pin_code\n",
    "    #     7) gender\n",
    "    #     8) age\n",
    "    #     9) aadhaar_generated\n",
    "    #     10) enrolment_rejected\n",
    "    #     11) residents_providing_email,\n",
    "    #     12) residents_providing_mobile_number\n",
    "    #\n",
    "    # You can download a copy of the aadhaar data that we are passing \n",
    "    # into this exercise below:\n",
    "    # https://www.dropbox.com/s/vn8t4uulbsfmalo/aadhaar_data.csv\n",
    "        \n",
    "    q = \"select gender, district, sum(aadhaar_generated) from aadhaar_data where Age > 50 group by district, gender order by gender\"\n",
    "\n",
    "    # Execute your SQL command against the pandas frame\n",
    "    aadhaar_solution = pandasql.sqldf(q.lower(), locals())\n",
    "    return aadhaar_solution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print aggregate_query(\"aadhaar_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arctic Monkeys\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def api_get_request(url):\n",
    "    # In this exercise, you want to call the last.fm API to get a list of the\n",
    "    # top artists in Spain.\n",
    "    #\n",
    "    # Once you've done this, return the name of the number 1 top artist in Spain.\n",
    "    data = requests.get(url).text\n",
    "    data = json.load(data)\n",
    "    #print(data['topartists']['artist'][0]['name'])\n",
    "    return data['topartists']['artist'][0]['name'] # return the top artist in Spain\n",
    "    \n",
    "def api_get_json(file_name):\n",
    "    with open(file_name) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        json_data.close()\n",
    "        return d['topartists']['artist'][0]['name']\n",
    "        #pprint(d)    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # url should be the url to the last.fm api call which\n",
    "    # will return find the top artists in Spain\n",
    "\n",
    "    url = \"\"\n",
    "    #print api_get_request(url) \n",
    "    print api_get_json('lastfm.json') \n",
    "\n",
    "#Good job! Arctic Monkeys is top artist in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lahmanID   playerID   managerID       hofID  birthYear  birthMonth  \\\n",
      "0           1  aaronha01         NaN  aaronha01h       1934           2   \n",
      "1           2  aaronto01         NaN         NaN       1939           8   \n",
      "2           3   aasedo01         NaN         NaN       1954           9   \n",
      "3           4   abadan01         NaN         NaN       1972           8   \n",
      "4           5  abadijo01         NaN         NaN       1854          11   \n",
      "5           6  abbated01         NaN         NaN       1877           4   \n",
      "6           7  abbeybe01         NaN         NaN       1869          11   \n",
      "7           8  abbeych01         NaN         NaN       1866          10   \n",
      "8           9  abbotda01         NaN         NaN       1862           3   \n",
      "9          10  abbotfr01         NaN         NaN       1874          10   \n",
      "10         11  abbotgl01         NaN         NaN       1951           2   \n",
      "11         12  abbotje01         NaN         NaN       1972           8   \n",
      "12         13  abbotji01         NaN  abbotji01h       1967           9   \n",
      "13         14  abbotku01         NaN         NaN       1969           6   \n",
      "14         15  abbotky01         NaN         NaN       1968           2   \n",
      "15         16  abbotod01         NaN         NaN       1888           9   \n",
      "16         17  abbotpa01         NaN         NaN       1967           9   \n",
      "17         18   aberal01         NaN         NaN       1927           7   \n",
      "18         19  abercda01         NaN         NaN       1851           1   \n",
      "19         20  abernbi01         NaN         NaN       1929           1   \n",
      "20         21  abernbr01         NaN         NaN       1977           9   \n",
      "21         22  abernte01         NaN         NaN       1921          10   \n",
      "22         23  abernte02         NaN         NaN       1933           3   \n",
      "23         24  abernwo01         NaN         NaN       1915           2   \n",
      "24         25  aberscl01         NaN         NaN       1921           8   \n",
      "25         26  ablesha01         NaN         NaN       1883          10   \n",
      "26         27  abnersh01         NaN         NaN       1966           6   \n",
      "27         28  abramca01         NaN         NaN       1924           3   \n",
      "28         29  abramge01         NaN         NaN       1899          11   \n",
      "29         30  abregjo01         NaN         NaN       1962           7   \n",
      "..        ...        ...         ...         ...        ...         ...   \n",
      "967       969  bergehe01         NaN         NaN       1882           1   \n",
      "968       970  bergejo01         NaN         NaN       1886          12   \n",
      "969       971  bergejo02         NaN         NaN       1901           8   \n",
      "970       972  bergetu01         NaN         NaN       1867          12   \n",
      "971       973  bergewa01         NaN  bergewa01h       1905          10   \n",
      "972       974  bergepe01         NaN         NaN       1977          11   \n",
      "973       975  berghjo01         NaN         NaN       1857          10   \n",
      "974       976  berghma01         NaN         NaN       1888           6   \n",
      "975       977  bergmal01         NaN         NaN       1890           9   \n",
      "976       978  bergmda01         NaN         NaN       1953           6   \n",
      "977       979  bergmse01         NaN         NaN       1970           4   \n",
      "978       980  berkefr01         NaN         NaN        NaN         NaN   \n",
      "979       981  berkena01         NaN         NaN       1831         NaN   \n",
      "980       982  berkmla01         NaN         NaN       1976           2   \n",
      "981       983  berlyja01         NaN         NaN       1903           5   \n",
      "982       984  bermabo01         NaN         NaN       1899           1   \n",
      "983       985  bernavi01         NaN         NaN       1953          10   \n",
      "984       986  bernacu01         NaN         NaN       1878           2   \n",
      "985       987  bernadw01         NaN         NaN       1952           5   \n",
      "986       988  bernajo01         NaN         NaN       1882           3   \n",
      "987       989  bernato01         NaN         NaN       1956           8   \n",
      "988       990  bernead01         NaN         NaN       1976          11   \n",
      "989       991  bernhbi01         NaN         NaN       1871           3   \n",
      "990       992  bernhju01         NaN         NaN       1953           8   \n",
      "991       993  bernhwa01         NaN         NaN       1893           5   \n",
      "992       994  bernica01         NaN         NaN       1927           1   \n",
      "993       995   berojo01         NaN         NaN       1922          12   \n",
      "994       996  berrada01         NaN         NaN       1956          12   \n",
      "995       997  berrayo01  berrayo01m  berrayo01h       1925           5   \n",
      "996       998  berrade01         NaN         NaN       1887          10   \n",
      "\n",
      "     birthDay birthCountry birthState             birthCity    ...      bats  \\\n",
      "0           5          USA         AL                Mobile    ...         R   \n",
      "1           5          USA         AL                Mobile    ...         R   \n",
      "2           8          USA         CA                Orange    ...         R   \n",
      "3          25          USA         FL       West Palm Beach    ...         L   \n",
      "4           4          USA         PA          Philadelphia    ...         R   \n",
      "5          15          USA         PA               Latrobe    ...         R   \n",
      "6          29          USA         VT                 Essex    ...         R   \n",
      "7          14          USA         NE            Falls City    ...         L   \n",
      "8          16          USA         OH               Portage    ...         R   \n",
      "9          22          USA         OH            Versailles    ...         R   \n",
      "10         16          USA         AR           Little Rock    ...         R   \n",
      "11         17          USA         GA               Atlanta    ...         R   \n",
      "12         19          USA         MI                 Flint    ...         L   \n",
      "13          2          USA         OH            Zanesville    ...         R   \n",
      "14         18          USA         MA           Newburyport    ...         L   \n",
      "15          5          USA         PA             New Eagle    ...         R   \n",
      "16         15          USA         CA              Van Nuys    ...         R   \n",
      "17         31          USA         OH             Cleveland    ...         L   \n",
      "18          2          USA         MD           Fort Towson    ...       NaN   \n",
      "19         30          USA         CA              Torrance    ...         R   \n",
      "20         23          USA         GA               Atlanta    ...         R   \n",
      "21         30          USA         NC                Mebane    ...         R   \n",
      "22          6          USA         NC               Stanley    ...         R   \n",
      "23          1          USA         NC           Forest City    ...         L   \n",
      "24         28          USA         IL               Chicago    ...         R   \n",
      "25          4          USA         TX               Terrell    ...         R   \n",
      "26         17          USA         OH              Hamilton    ...         R   \n",
      "27          2          USA         PA          Philadelphia    ...         L   \n",
      "28          9          USA         WA               Seattle    ...         R   \n",
      "29          4          USA         TX        Corpus Christi    ...         R   \n",
      "..        ...          ...        ...                   ...    ...       ...   \n",
      "967         7          USA         IL               LaSalle    ...       NaN   \n",
      "968        20          USA         MO             St. Louis    ...         R   \n",
      "969        27          USA         PA          Philadelphia    ...         R   \n",
      "970         6          USA         PA            Pittsburgh    ...       NaN   \n",
      "971        10          USA         IL               Chicago    ...         R   \n",
      "972         9          USA         MA            Greenfield    ...         L   \n",
      "973         8          USA         MA                Boston    ...       NaN   \n",
      "974        18          USA         PA               Elliott    ...         L   \n",
      "975        27          USA         IN                  Peru    ...         R   \n",
      "976         6          USA         IL              Evanston    ...         L   \n",
      "977        11          USA         IL                Joliet    ...         R   \n",
      "978       NaN          NaN        NaN                   NaN    ...       NaN   \n",
      "979       NaN          USA         PA                   NaN    ...       NaN   \n",
      "980        10          USA         TX                  Waco    ...         B   \n",
      "981        24          USA         LA          Natchitoches    ...         R   \n",
      "982        24          USA         NY              New York    ...         R   \n",
      "983         6          USA         CA           Los Angeles    ...         R   \n",
      "984        18          USA         WV           Parkersburg    ...         L   \n",
      "985        31          USA         IL          Mount Vernon    ...         R   \n",
      "986        24          USA         IL              Brighton    ...         R   \n",
      "987        24         P.R.        NaN                Caguas    ...         B   \n",
      "988        28          USA         CA              San Jose    ...         R   \n",
      "989        16          USA         NY              Clarence    ...         B   \n",
      "990        31         D.R.        NaN  San Pedro de Macoris    ...         R   \n",
      "991        20          USA         PA      Pleasant Village    ...         R   \n",
      "992        28         P.R.        NaN            Juana Diaz    ...         R   \n",
      "993        22          USA         WV                  Gary    ...         L   \n",
      "994        13          USA         NJ             Ridgewood    ...         R   \n",
      "995        12          USA         MO             St. Louis    ...         L   \n",
      "996         8          USA         MA              Merrimac    ...         L   \n",
      "\n",
      "     throws       debut   finalGame              college lahman40ID  \\\n",
      "0         R   4/13/1954   10/3/1976                  NaN  aaronha01   \n",
      "1         R   4/10/1962   9/26/1971                  NaN  aaronto01   \n",
      "2         R   7/26/1977   10/3/1990    Cal St. Fullerton   aasedo01   \n",
      "3         L   9/10/2001   4/13/2006    Middle Georgia JC   abadan01   \n",
      "4         R   4/26/1875   6/10/1875                  NaN  abadijo01   \n",
      "5         R    9/4/1897   9/15/1910                  NaN  abbated01   \n",
      "6         R   6/14/1892   9/23/1896                  NaN  abbeybe01   \n",
      "7       NaN   8/16/1893   8/19/1897                  NaN  abbeych01   \n",
      "8         R   4/19/1890   5/23/1890                  NaN  abbotda01   \n",
      "9         R   4/25/1903   9/20/1905                  NaN  abbotfr01   \n",
      "10        R   7/29/1973    8/8/1984       Arkansas State  abbotgl01   \n",
      "11        L   6/10/1997   9/29/2001             Kentucky  abbotje01   \n",
      "12        L    4/8/1989   7/21/1999             Michigan  abbotji01   \n",
      "13        R    9/8/1993   4/13/2001                  NaN  abbotku01   \n",
      "14        L   9/10/1991   8/24/1996   Cal St. Long Beach  abbotky01   \n",
      "15        R   9/10/1910  10/15/1910                  NaN  abbotod01   \n",
      "16        R   8/21/1990    8/7/2004                  NaN  abbotpa01   \n",
      "17        L   9/15/1950   9/11/1957                  NaN   aberal01   \n",
      "18      NaN  10/21/1871  10/21/1871                  NaN  abercda01   \n",
      "19        R   9/27/1952   9/27/1952                  NaN  abernbi01   \n",
      "20        R   6/25/2001   9/29/2005                  NaN  abernbr01   \n",
      "21        L   9/19/1942   4/29/1944                  NaN  abernte01   \n",
      "22        R   4/13/1955   9/30/1972                  NaN  abernte02   \n",
      "23        L   7/28/1946   4/17/1947                  NaN  abernwo01   \n",
      "24        R   7/18/1947    5/9/1949                  NaN  aberscl01   \n",
      "25        L    9/4/1905    5/5/1911                  NaN  ablesha01   \n",
      "26        R    9/8/1987   10/3/1992                  NaN  abnersh01   \n",
      "27        L   4/20/1949    5/9/1956                  NaN  abramca01   \n",
      "28        R   4/19/1923    5/4/1923                  NaN  abramge01   \n",
      "29        R    9/4/1985   10/3/1985                  NaN  abregjo01   \n",
      "..      ...         ...         ...                  ...        ...   \n",
      "967       R    5/6/1907   7/22/1910                  NaN  bergehe01   \n",
      "968       R   4/11/1913   9/27/1914                  NaN  bergejo01   \n",
      "969       R   4/20/1922    9/6/1927                  NaN  bergejo02   \n",
      "970       R    5/9/1890   8/28/1892                  NaN  bergetu01   \n",
      "971       R   4/15/1930    7/4/1940                  NaN  bergewa01   \n",
      "972       R    9/7/1999   4/18/2004                  NaN  bergepe01   \n",
      "973     NaN    8/5/1876         NaN                  NaN  berghjo01   \n",
      "974       R    9/8/1911   10/3/1915                  NaN  berghma01   \n",
      "975       R   8/29/1916   9/12/1916                  NaN  bergmal01   \n",
      "976       L   8/26/1975   10/4/1992       Illinois State  bergmda01   \n",
      "977       R    7/7/1993   6/17/2000                  NaN  bergmse01   \n",
      "978     NaN    7/4/1884   7/12/1884                  NaN  berkefr01   \n",
      "979     NaN  10/30/1871  10/30/1871                  NaN  berkena01   \n",
      "980       L   7/16/1999         NaN                 Rice  berkmla01   \n",
      "981       R   4/22/1924   9/24/1933                  NaN  berlyja01   \n",
      "982       R    6/4/1918   6/12/1918                  NaN  bermabo01   \n",
      "983       R    4/6/1977   5/17/1977      Cal Poly Pomona  bernavi01   \n",
      "984       R   9/17/1900   6/24/1901                  NaN  bernacu01   \n",
      "985       R   6/29/1978   10/2/1982              Belmont  bernadw01   \n",
      "986       R   9/23/1909   9/23/1909                  NaN  bernajo01   \n",
      "987       R   7/13/1979   4/26/1991              Florida  bernato01   \n",
      "988       R    8/1/2000   9/30/2006  Armstrong State, GA  bernead01   \n",
      "989       R   4/24/1899   9/19/1907                  NaN  bernhbi01   \n",
      "990       R   7/10/1976    4/8/1979                  NaN  bernhju01   \n",
      "991       R   7/16/1918   7/16/1918                  NaN  bernhwa01   \n",
      "992       R   4/22/1953   9/22/1953                  NaN  bernica01   \n",
      "993       R   9/26/1948   7/22/1951                  NaN   berojo01   \n",
      "994       R   8/22/1977   10/4/1987                  NaN  berrada01   \n",
      "995       R   9/22/1946    5/9/1965                  NaN  berrayo01   \n",
      "996       L   8/11/1912   8/11/1912                  NaN  berrade01   \n",
      "\n",
      "    lahman45ID   retroID    holtzID    bbrefID  \n",
      "0    aaronha01  aaroh101  aaronha01  aaronha01  \n",
      "1    aaronto01  aarot101  aaronto01  aaronto01  \n",
      "2     aasedo01  aased001   aasedo01   aasedo01  \n",
      "3     abadan01  abada001   abadan01   abadan01  \n",
      "4    abadijo01  abadj101  abadijo01  abadijo01  \n",
      "5    abbated01  abbae101  abbated01  abbated01  \n",
      "6    abbeybe01  abbeb101  abbeybe01  abbeybe01  \n",
      "7    abbeych01  abbec101  abbeych01  abbeych01  \n",
      "8    abbotda01  abbod101  abbotda01  abbotda01  \n",
      "9    abbotfr01  abbof101  abbotfr01  abbotfr01  \n",
      "10   abbotgl01  abbog001  abbotgl01  abbotgl01  \n",
      "11   abbotje01  abboj002  abbotje01  abbotje01  \n",
      "12   abbotji01  abboj001  abbotji01  abbotji01  \n",
      "13   abbotku01  abbok002  abbotku01  abbotku01  \n",
      "14   abbotky01  abbok001  abbotky01  abbotky01  \n",
      "15   abbotod01  abboo101  abbotod01  abbotod01  \n",
      "16   abbotpa01  abbop001  abbotpa01  abbotpa01  \n",
      "17    aberal01  abera101   aberal01   aberal01  \n",
      "18   abercda01  aberd101  abercda01  abercda01  \n",
      "19   abernbi01  aberb101  abernbi01  abernbi01  \n",
      "20   abernbr01  aberb001  abernbr01  abernbr01  \n",
      "21   abernte01  abert102  abernte01  abernte01  \n",
      "22   abernte02  abert101  abernte02  abernte02  \n",
      "23   abernwo01  aberw101  abernwo01  abernwo01  \n",
      "24   aberscl01  aberc101  aberscl01  aberscl01  \n",
      "25   ablesha01  ableh101  ablesha01  ablesha01  \n",
      "26   abnersh01  abnes001  abnersh01  abnersh01  \n",
      "27   abramca01  abrac101  abramca01  abramca01  \n",
      "28   abramge01  abrag101  abramge01  abramge01  \n",
      "29   abregjo01  abrej001  abregjo01  abregjo01  \n",
      "..         ...       ...        ...        ...  \n",
      "967  bergehe01  bergh101  bergehe01  bergehe01  \n",
      "968  bergejo01  bergj103  bergejo01  bergejo01  \n",
      "969  bergejo02  bergj102  bergejo02  bergejo02  \n",
      "970  bergetu01  bergt101  bergetu01  bergetu01  \n",
      "971  bergewa01  bergw101  bergewa01  bergewa01  \n",
      "972  bergepe01  bergp001  bergepe01  bergepe01  \n",
      "973  berghjo01  bergj101  berghjo01  berghjo01  \n",
      "974  berghma01  bergm101  berghma01  berghma01  \n",
      "975  bergmal01  berga101  bergmal01  bergmal01  \n",
      "976  bergmda01  bergd001  bergmda01  bergmda01  \n",
      "977  bergmse01  bergs001  bergmse01  bergmse01  \n",
      "978  berkefr01  berkf101  berkefr01  berkefr01  \n",
      "979  berkena01  berkn101  berkena01  berkena01  \n",
      "980  berkmla01  berkl001  berkmla01  berkmla01  \n",
      "981  berlyja01  berlj101  berlyja01  berlyja01  \n",
      "982  bermabo01  bermb101  bermabo01  bermabo01  \n",
      "983  bernavi01  bernv101  bernavi01  bernavi01  \n",
      "984  bernacu01  bernc101  bernacu01  bernacu01  \n",
      "985  bernadw01  bernd101  bernadw01  bernadw01  \n",
      "986  bernajo01  bernj102  bernajo01  bernajo01  \n",
      "987  bernato01  bernt001  bernato01  bernato01  \n",
      "988  bernead01  berna001  bernead01  bernead01  \n",
      "989  bernhbi01  bernb101  bernhbi01  bernhbi01  \n",
      "990  bernhju01  bernj101  bernhju01  bernhju01  \n",
      "991  bernhwa01  bernw101  bernhwa01  bernhwa01  \n",
      "992  bernica01  bernc102  bernica01  bernica01  \n",
      "993   berojo01  beroj101   berojo01   berojo01  \n",
      "994  berrada01  berrd001  berrada01  berrada01  \n",
      "995  berrayo01  berry101  berrayo01  berrayo01  \n",
      "996  berrade01  berrd101  berrade01  berrade01  \n",
      "\n",
      "[997 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import *\n",
    "import numpy\n",
    "\n",
    "def imputation(filename):\n",
    "    # Pandas dataframes have a method called 'fillna(value)', such that you can\n",
    "    # pass in a single value to replace any NAs in a dataframe or series. You\n",
    "    # can call it like this: \n",
    "    #     dataframe['column'] = dataframe['column'].fillna(value)\n",
    "    #\n",
    "    # Using the numpy.mean function, which calculates the mean of a numpy\n",
    "    # array, impute any missing values in our Lahman baseball\n",
    "    # data sets 'weight' column by setting them equal to the average weight.\n",
    "    # \n",
    "    # You can access the 'weight' colum in the baseball data frame by\n",
    "    # calling baseball['weight']\n",
    "\n",
    "    baseball = pandas.read_csv(filename)\n",
    "    \n",
    "    #YOUR CODE GOES HERE\n",
    "    baseball['weight']=baseball['weight'].fillna(numpy.mean(baseball['weight']))\n",
    "\n",
    "    return baseball\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    baseball_weight=imputation(\"Master.csv\")\n",
    "    print baseball_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(rain)\n",
      "0           10\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "\n",
    "def num_rainy_days(filename):\n",
    "    '''\n",
    "    This function should run a SQL query on a dataframe of\n",
    "    weather data.  The SQL query should return one column and\n",
    "    one row - a count of the number of days in the dataframe where\n",
    "    the rain column is equal to 1 (i.e., the number of days it\n",
    "    rained).  The dataframe will be titled 'weather_data'. You'll\n",
    "    need to provide the SQL query.  You might find SQL's count function\n",
    "    useful for this exercise.  You can read more about it here:\n",
    "    \n",
    "    https://dev.mysql.com/doc/refman/5.1/en/counting-rows.html\n",
    "    \n",
    "    You might also find that interpreting numbers as integers or floats may not\n",
    "    work initially.  In order to get around this issue, it may be useful to cast\n",
    "    these numbers as integers.  This can be done by writing cast(column as integer).\n",
    "    So for example, if we wanted to cast the maxtempi column as an integer, we would actually\n",
    "    write something like where cast(maxtempi as integer) = 76, as opposed to simply \n",
    "    where maxtempi = 76.\n",
    "    \n",
    "    You can see the weather data that we are passing in below:\n",
    "    https://www.dropbox.com/s/7sf0yqc9ykpq3w8/weather_underground.csv\n",
    "    '''\n",
    "    weather_data = pandas.read_csv(filename)\n",
    "\n",
    "    q = \"\"\"\n",
    "    select count(rain) from weather_data where rain > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    #Execute your SQL command against the pandas frame\n",
    "    rainy_days = pandasql.sqldf(q.lower(), locals())\n",
    "    return rainy_days\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print num_rainy_days(\"weather_underground.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fog  maxtempi\n",
      "0    0        86\n",
      "1    1        81\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "\n",
    "def max_temp_aggregate_by_fog(filename):\n",
    "    '''\n",
    "    This function should run a SQL query on a dataframe of\n",
    "    weather data.  The SQL query should return two columns and\n",
    "    two rows - whether it was foggy or not (0 or 1) and the max\n",
    "    maxtempi for that fog value (i.e., the maximum max temperature\n",
    "    for both foggy and non-foggy days).  The dataframe will be \n",
    "    titled 'weather_data'. You'll need to provide the SQL query.\n",
    "    \n",
    "    You might also find that interpreting numbers as integers or floats may not\n",
    "    work initially.  In order to get around this issue, it may be useful to cast\n",
    "    these numbers as integers.  This can be done by writing cast(column as integer).\n",
    "    So for example, if we wanted to cast the maxtempi column as an integer, we would actually\n",
    "    write something like where cast(maxtempi as integer) = 76, as opposed to simply \n",
    "    where maxtempi = 76.\n",
    "    \n",
    "    You can see the weather data that we are passing in below:\n",
    "    https://www.dropbox.com/s/7sf0yqc9ykpq3w8/weather_underground.csv\n",
    "    '''\n",
    "    weather_data = pandas.read_csv(filename)\n",
    "\n",
    "    q = \"\"\"\n",
    "    select fog, maxtempi from weather_data group by fog\n",
    "    \"\"\"\n",
    "    \n",
    "    #Execute your SQL command against the pandas frame\n",
    "    foggy_days = pandasql.sqldf(q.lower(), locals())\n",
    "    return foggy_days\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"weather_underground.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    student_df = max_temp_aggregate_by_fog(input_filename)\n",
    "    print student_df\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg(cast(meantempi as integer))\n",
      "0                        65.111111\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "def avg_weekend_temperature(filename):\n",
    "    '''\n",
    "    This function should run a SQL query on a dataframe of\n",
    "    weather data.  The SQL query should return one column and\n",
    "    one row - the average meantempi on days that are a Saturday\n",
    "    or Sunday (i.e., the the average mean temperature on weekends).\n",
    "    The dataframe will be titled 'weather_data' and you can access\n",
    "    the date in the dataframe via the 'date' column.\n",
    "    \n",
    "    You'll need to provide  the SQL query.\n",
    "    \n",
    "    You might also find that interpreting numbers as integers or floats may not\n",
    "    work initially.  In order to get around this issue, it may be useful to cast\n",
    "    these numbers as integers.  This can be done by writing cast(column as integer).\n",
    "    So for example, if we wanted to cast the maxtempi column as an integer, we would actually\n",
    "    write something like where cast(maxtempi as integer) = 76, as opposed to simply \n",
    "    where maxtempi = 76.\n",
    "    \n",
    "    Also, you can convert dates to days of the week via the 'strftime' keyword in SQL.\n",
    "    For example, cast (strftime('%w', date) as integer) will return 0 if the date\n",
    "    is a Sunday or 6 if the date is a Saturday.\n",
    "    \n",
    "    You can see the weather data that we are passing in below:\n",
    "    https://www.dropbox.com/s/7sf0yqc9ykpq3w8/weather_underground.csv\n",
    "    '''\n",
    "    weather_data = pandas.read_csv(filename)\n",
    "\n",
    "    q = \"\"\"\n",
    "    select avg(cast(meantempi as integer)) from weather_data where cast(strftime('%w', date) as integer) = 0 OR cast(strftime('%w', date) as integer) =6\n",
    "    \"\"\"\n",
    "    \n",
    "    #Execute your SQL command against the pandas frame\n",
    "    mean_temp_weekends = pandasql.sqldf(q.lower(), locals())\n",
    "    return mean_temp_weekends\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print avg_weekend_temperature(\"weather_underground.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg(cast(mintempi as integer))\n",
      "0                           61.25\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandasql\n",
    "\n",
    "def avg_min_temperature(filename):\n",
    "    '''\n",
    "    This function should run a SQL query on a dataframe of\n",
    "    weather data. More specifically you want to find the average\n",
    "    minimum temperature (mintempi column of the weather dataframe) on \n",
    "    rainy days where the minimum temperature is greater than 55 degrees.\n",
    "    \n",
    "    You might also find that interpreting numbers as integers or floats may not\n",
    "    work initially.  In order to get around this issue, it may be useful to cast\n",
    "    these numbers as integers.  This can be done by writing cast(column as integer).\n",
    "    So for example, if we wanted to cast the maxtempi column as an integer, we would actually\n",
    "    write something like where cast(maxtempi as integer) = 76, as opposed to simply \n",
    "    where maxtempi = 76.\n",
    "    \n",
    "    You can see the weather data that we are passing in below:\n",
    "    https://www.dropbox.com/s/7sf0yqc9ykpq3w8/weather_underground.csv\n",
    "    '''\n",
    "    weather_data = pandas.read_csv(filename)\n",
    "\n",
    "    q = \"\"\"\n",
    "    select avg(cast(mintempi as integer)) from weather_data where cast(rain as integer) = 1 and cast(mintempi as integer) > 55\n",
    "    \"\"\"\n",
    "    \n",
    "    #Execute your SQL command against the pandas frame\n",
    "    avg_min_temp_rainy = pandasql.sqldf(q.lower(), locals())\n",
    "    return avg_min_temp_rainy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"weather_underground.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    student_df = avg_min_temperature(input_filename)\n",
    "    print student_df\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def fix_turnstile_data(filenames):\n",
    "    '''\n",
    "    Filenames is a list of MTA Subway turnstile text files. A link to an example\n",
    "    MTA Subway turnstile text file can be seen at the URL below:\n",
    "    http://web.mta.info/developers/data/nyct/turnstile/turnstile_110507.txt\n",
    "    \n",
    "    As you can see, there are numerous data points included in each row of the\n",
    "    a MTA Subway turnstile text file. \n",
    "\n",
    "    You want to write a function that will update each row in the text\n",
    "    file so there is only one entry per row. A few examples below:\n",
    "    A002,R051,02-00-00,05-28-11,00:00:00,REGULAR,003178521,001100739\n",
    "    A002,R051,02-00-00,05-28-11,04:00:00,REGULAR,003178541,001100746\n",
    "    A002,R051,02-00-00,05-28-11,08:00:00,REGULAR,003178559,001100775\n",
    "    \n",
    "    Write the updates to a different text file in the format of \"updated_\" + filename.\n",
    "    For example:\n",
    "        1) if you read in a text file called \"turnstile_110521.txt\"\n",
    "        2) you should write the updated data to \"updated_turnstile_110521.txt\"\n",
    "\n",
    "    The order of the fields should be preserved. Remember to read through the \n",
    "    Instructor Notes below for more details on the task. \n",
    "    \n",
    "    In addition, here is a CSV reader/writer introductory tutorial:\n",
    "    http://goo.gl/HBbvyy\n",
    "    \n",
    "    You can see a sample of the turnstile text file that's passed into this function\n",
    "    and the the corresponding updated file in the links below:\n",
    "    \n",
    "    Sample input file:\n",
    "    https://www.dropbox.com/s/mpin5zv4hgrx244/turnstile_110528.txt\n",
    "    Sample updated file:\n",
    "    https://www.dropbox.com/s/074xbgio4c39b7h/solution_turnstile_110528.txt\n",
    "    '''\n",
    "    for name in filenames:\n",
    "        # your code here\n",
    "        f_in=open(name,'r')\n",
    "        f_out=open('updated_'+name,'w')\n",
    "        \n",
    "        reader_in=csv.reader(f_in, delimiter=',')\n",
    "        writer_out=csv.writer(f_out, delimiter=',')\n",
    "        \n",
    "        for line in reader_in:\n",
    "            for x in xrange((len(line)-3)/5):\n",
    "                line_1 = [line[0], line[1], line[2], line[5*x+3], line[5*x+4], line[5*x+5], line[5*x+6], line[5*x+7]]\n",
    "                writer_out.writerow(line_1)\n",
    "            \n",
    "        f_in.close()\n",
    "        f_out.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = ['turnstile_110528.txt', 'turnstile_110604.txt']\n",
    "    fix_turnstile_data(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_master_turnstile_file(filenames, output_file):\n",
    "    '''\n",
    "    Write a function that takes the files in the list filenames, which all have the \n",
    "    columns 'C/A, UNIT, SCP, DATEn, TIMEn, DESCn, ENTRIESn, EXITSn', and consolidates\n",
    "    them into one file located at output_file.  There should be ONE row with the column\n",
    "    headers, located at the top of the file. The input files do not have column header\n",
    "    rows of their own.\n",
    "    \n",
    "    For example, if file_1 has:\n",
    "    line 1 ...\n",
    "    line 2 ...\n",
    "    \n",
    "    and another file, file_2 has:\n",
    "    line 3 ...\n",
    "    line 4 ...\n",
    "    line 5 ...\n",
    "    \n",
    "    We need to combine file_1 and file_2 into a master_file like below:\n",
    "     'C/A, UNIT, SCP, DATEn, TIMEn, DESCn, ENTRIESn, EXITSn'\n",
    "    line 1 ...\n",
    "    line 2 ...\n",
    "    line 3 ...\n",
    "    line 4 ...\n",
    "    line 5 ...\n",
    "    '''\n",
    "    with open(output_file, 'w') as master_file:\n",
    "        master_file.write('C/A,UNIT,SCP,DATEn,TIMEn,DESCn,ENTRIESn,EXITSn\\n')\n",
    "        for filename in filenames:\n",
    "            # your code here\n",
    "            f_in=open(filename,'r')\n",
    "            reader_in=f_in.read()\n",
    "            master_file.write(reader_in)\n",
    "            f_in.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = ['turnstile_110528.txt', 'turnstile_110604.txt']\n",
    "    output = \"turnstile_data_master.csv\"\n",
    "    create_master_turnstile_file(input_files, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0   C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn  \\\n",
      "0              0  A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   \n",
      "1              1  A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   \n",
      "2              2  A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   \n",
      "3              3  A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   \n",
      "4              4  A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   \n",
      "5              5  A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   \n",
      "6              6  A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   \n",
      "7              7  A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   \n",
      "8              8  A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   \n",
      "9              9  A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   \n",
      "10            10  A002  R051  02-00-00  05-02-11  16:00:00  REGULAR   3145337   \n",
      "11            11  A002  R051  02-00-00  05-02-11  20:00:00  REGULAR   3146168   \n",
      "12            12  A002  R051  02-00-00  05-03-11  00:00:00  REGULAR   3146322   \n",
      "13            13  A002  R051  02-00-00  05-03-11  04:00:00  REGULAR   3146335   \n",
      "14            14  A002  R051  02-00-00  05-03-11  08:00:00  REGULAR   3146371   \n",
      "15            15  A002  R051  02-00-00  05-03-11  12:00:00  REGULAR   3146510   \n",
      "16            16  A002  R051  02-00-00  05-03-11  16:00:00  REGULAR   3146790   \n",
      "17            17  A002  R051  02-00-00  05-03-11  20:00:00  REGULAR   3147615   \n",
      "18            18  A002  R051  02-00-00  05-04-11  00:00:00  REGULAR   3147798   \n",
      "19            19  A002  R051  02-00-00  05-04-11  04:00:00  REGULAR   3147809   \n",
      "20            20  A002  R051  02-00-00  05-04-11  08:00:00  REGULAR   3147859   \n",
      "21            21  A002  R051  02-00-00  05-04-11  12:00:00  REGULAR   3147999   \n",
      "22            22  A002  R051  02-00-00  05-04-11  16:00:00  REGULAR   3148276   \n",
      "23            23  A002  R051  02-00-00  05-04-11  20:00:00  REGULAR   3149108   \n",
      "24            24  A002  R051  02-00-00  05-05-11  00:00:00  REGULAR   3149281   \n",
      "25            25  A002  R051  02-00-00  05-05-11  04:00:00  REGULAR   3149297   \n",
      "26            26  A002  R051  02-00-00  05-05-11  08:00:00  REGULAR   3149331   \n",
      "27            34  A002  R051  02-00-00  05-05-11  20:00:00  REGULAR   3150639   \n",
      "28            35  A002  R051  02-00-00  05-06-11  00:00:00  REGULAR   3150815   \n",
      "29            36  A002  R051  02-00-00  05-06-11  04:00:00  REGULAR   3150840   \n",
      "...          ...   ...   ...       ...       ...       ...      ...       ...   \n",
      "4202        4956  A034  R170  03-06-00  05-01-11  09:00:00  REGULAR   9164155   \n",
      "4203        4957  A034  R170  03-06-00  05-01-11  13:00:00  REGULAR   9164311   \n",
      "4204        4958  A034  R170  03-06-00  05-01-11  17:00:00  REGULAR   9164569   \n",
      "4205        4959  A034  R170  03-06-00  05-01-11  21:00:00  REGULAR   9164769   \n",
      "4206        4960  A034  R170  03-06-00  05-02-11  01:00:00  REGULAR   9164854   \n",
      "4207        4961  A034  R170  03-06-00  05-02-11  05:00:00  REGULAR   9164859   \n",
      "4208        4962  A034  R170  03-06-00  05-02-11  09:00:00  REGULAR   9164941   \n",
      "4209        4963  A034  R170  03-06-00  05-02-11  13:00:00  REGULAR   9165124   \n",
      "4210        4964  A034  R170  03-06-00  05-02-11  17:00:00  REGULAR   9165461   \n",
      "4211        4965  A034  R170  03-06-00  05-02-11  21:00:00  REGULAR   9165857   \n",
      "4212        4966  A034  R170  03-06-00  05-03-11  01:00:00  REGULAR   9165949   \n",
      "4213        4967  A034  R170  03-06-00  05-03-11  05:00:00  REGULAR   9165956   \n",
      "4214        4968  A034  R170  03-06-00  05-03-11  09:00:00  REGULAR   9166043   \n",
      "4215        4974  A034  R170  03-06-00  05-03-11  21:00:00  REGULAR   9167110   \n",
      "4216        4975  A034  R170  03-06-00  05-04-11  01:00:00  REGULAR   9167276   \n",
      "4217        4976  A034  R170  03-06-00  05-04-11  05:00:00  REGULAR   9167293   \n",
      "4218        4977  A034  R170  03-06-00  05-04-11  09:00:00  REGULAR   9167375   \n",
      "4219        4978  A034  R170  03-06-00  05-04-11  13:00:00  REGULAR   9167531   \n",
      "4220        4979  A034  R170  03-06-00  05-04-11  17:00:00  REGULAR   9167797   \n",
      "4221        4980  A034  R170  03-06-00  05-04-11  21:00:00  REGULAR   9168228   \n",
      "4222        4981  A034  R170  03-06-00  05-05-11  01:00:00  REGULAR   9168382   \n",
      "4223        4982  A034  R170  03-06-00  05-05-11  05:00:00  REGULAR   9168395   \n",
      "4224        4983  A034  R170  03-06-00  05-05-11  09:00:00  REGULAR   9168483   \n",
      "4225        4984  A034  R170  03-06-00  05-05-11  13:00:00  REGULAR   9168683   \n",
      "4226        4985  A034  R170  03-06-00  05-05-11  17:00:00  REGULAR   9169064   \n",
      "4227        4986  A034  R170  03-06-00  05-05-11  21:00:00  REGULAR   9169547   \n",
      "4228        4991  A034  R170  03-06-00  05-06-11  05:00:00  REGULAR   9169787   \n",
      "4229        4995  A034  R170  03-06-00  05-06-11  21:00:00  REGULAR   9171025   \n",
      "4230        4996  A034  R170  03-06-01  05-01-11  01:00:00  REGULAR   5604407   \n",
      "4231        4997  A034  R170  03-06-01  05-01-11  05:00:00  REGULAR   5604511   \n",
      "\n",
      "       EXITSn  ENTRIESn_hourly  EXITSn_hourly  \n",
      "0     1088151                0              0  \n",
      "1     1088159               23              8  \n",
      "2     1088177               18             18  \n",
      "3     1088231               71             54  \n",
      "4     1088275              170             44  \n",
      "5     1088317              214             42  \n",
      "6     1088328               87             11  \n",
      "7     1088331               10              3  \n",
      "8     1088420               36             89  \n",
      "9     1088753              153            333  \n",
      "10    1088823              243             70  \n",
      "11    1088888              831             65  \n",
      "12    1088918              154             30  \n",
      "13    1088921               13              3  \n",
      "14    1089014               36             93  \n",
      "15    1089341              139            327  \n",
      "16    1089417              280             76  \n",
      "17    1089478              825             61  \n",
      "18    1089515              183             37  \n",
      "19    1089520               11              5  \n",
      "20    1089632               50            112  \n",
      "21    1089965              140            333  \n",
      "22    1090054              277             89  \n",
      "23    1090117              832             63  \n",
      "24    1090139              173             22  \n",
      "25    1090145               16              6  \n",
      "26    1090257               34            112  \n",
      "27    1090714             1308            457  \n",
      "28    1090750              176             36  \n",
      "29    1090759               25              9  \n",
      "...       ...              ...            ...  \n",
      "4202  5796594               17              3  \n",
      "4203  5796677              156             83  \n",
      "4204  5796765              258             88  \n",
      "4205  5796838              200             73  \n",
      "4206  5796848               85             10  \n",
      "4207  5796849                5              1  \n",
      "4208  5796900               82             51  \n",
      "4209  5796959              183             59  \n",
      "4210  5797136              337            177  \n",
      "4211  5797317              396            181  \n",
      "4212  5797327               92             10  \n",
      "4213  5797327                7              0  \n",
      "4214  5797368               87             41  \n",
      "4215  5797892             1067            524  \n",
      "4216  5797906              166             14  \n",
      "4217  5797907               17              1  \n",
      "4218  5797946               82             39  \n",
      "4219  5798020              156             74  \n",
      "4220  5798161              266            141  \n",
      "4221  5798360              431            199  \n",
      "4222  5798378              154             18  \n",
      "4223  5798380               13              2  \n",
      "4224  5798410               88             30  \n",
      "4225  5798510              200            100  \n",
      "4226  5798683              381            173  \n",
      "4227  5798956              483            273  \n",
      "4228  5799002              240             46  \n",
      "4229  5799732             1238            730  \n",
      "4230  2448624                0              0  \n",
      "4231  2448644              104             20  \n",
      "\n",
      "[4232 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "\n",
    "def filter_by_regular(filename):\n",
    "    '''\n",
    "    This function should read the csv file located at filename into a pandas dataframe,\n",
    "    and filter the dataframe to only rows where the 'DESCn' column has the value 'REGULAR'.\n",
    "    \n",
    "    For example, if the pandas dataframe is as follows:\n",
    "    ,C/A,UNIT,SCP,DATEn,TIMEn,DESCn,ENTRIESn,EXITSn\n",
    "    0,A002,R051,02-00-00,05-01-11,00:00:00,REGULAR,3144312,1088151\n",
    "    1,A002,R051,02-00-00,05-01-11,04:00:00,DOOR,3144335,1088159\n",
    "    2,A002,R051,02-00-00,05-01-11,08:00:00,REGULAR,3144353,1088177\n",
    "    3,A002,R051,02-00-00,05-01-11,12:00:00,DOOR,3144424,1088231\n",
    "    \n",
    "    The dataframe will look like below after filtering to only rows where DESCn column\n",
    "    has the value 'REGULAR':\n",
    "    0,A002,R051,02-00-00,05-01-11,00:00:00,REGULAR,3144312,1088151\n",
    "    2,A002,R051,02-00-00,05-01-11,08:00:00,REGULAR,3144353,1088177\n",
    "    '''\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        reader_in=csv.reader(f, skipinitialspace=False,delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "        reader_in.next()\n",
    "        buf=[]\n",
    "        for line in reader_in:\n",
    "            wdb=[]\n",
    "            for index, wd in enumerate(line):\n",
    "                if index > 5:\n",
    "                    wd=int(wd.strip())\n",
    "                wdb.append(wd)\n",
    "            buf.append(wdb)\n",
    "        #print buf\n",
    "        indx=[]\n",
    "        for x in xrange(len(buf)):\n",
    "            indx.append(x)\n",
    "        #print indx\n",
    "        #print len(buf)\n",
    "    turnstile_data = pandas.DataFrame(data=buf, index=indx, \n",
    "                                          columns=['C/A','UNIT','SCP','DATEn','TIMEn','DESCn','ENTRIESn','EXITSn'])\n",
    "        # more of your code here\n",
    "    turnstile_data = turnstile_data[turnstile_data['DESCn']=='REGULAR']\n",
    "        #print turnstile_data\n",
    "    return turnstile_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_subset_get_hours_entries.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_df = turnstile_master.groupby(['C/A','UNIT','SCP']).apply(get_hourly_exits)\n",
    "    print student_df\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0   C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn  \\\n",
      "0              0  A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   \n",
      "1              1  A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   \n",
      "2              2  A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   \n",
      "3              3  A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   \n",
      "4              4  A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   \n",
      "5              5  A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   \n",
      "6              6  A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   \n",
      "7              7  A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   \n",
      "8              8  A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   \n",
      "9              9  A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   \n",
      "10            10  A002  R051  02-00-00  05-02-11  16:00:00  REGULAR   3145337   \n",
      "11            11  A002  R051  02-00-00  05-02-11  20:00:00  REGULAR   3146168   \n",
      "12            12  A002  R051  02-00-00  05-03-11  00:00:00  REGULAR   3146322   \n",
      "13            13  A002  R051  02-00-00  05-03-11  04:00:00  REGULAR   3146335   \n",
      "14            14  A002  R051  02-00-00  05-03-11  08:00:00  REGULAR   3146371   \n",
      "15            15  A002  R051  02-00-00  05-03-11  12:00:00  REGULAR   3146510   \n",
      "16            16  A002  R051  02-00-00  05-03-11  16:00:00  REGULAR   3146790   \n",
      "17            17  A002  R051  02-00-00  05-03-11  20:00:00  REGULAR   3147615   \n",
      "18            18  A002  R051  02-00-00  05-04-11  00:00:00  REGULAR   3147798   \n",
      "19            19  A002  R051  02-00-00  05-04-11  04:00:00  REGULAR   3147809   \n",
      "20            20  A002  R051  02-00-00  05-04-11  08:00:00  REGULAR   3147859   \n",
      "21            21  A002  R051  02-00-00  05-04-11  12:00:00  REGULAR   3147999   \n",
      "22            22  A002  R051  02-00-00  05-04-11  16:00:00  REGULAR   3148276   \n",
      "23            23  A002  R051  02-00-00  05-04-11  20:00:00  REGULAR   3149108   \n",
      "24            24  A002  R051  02-00-00  05-05-11  00:00:00  REGULAR   3149281   \n",
      "25            25  A002  R051  02-00-00  05-05-11  04:00:00  REGULAR   3149297   \n",
      "26            26  A002  R051  02-00-00  05-05-11  08:00:00  REGULAR   3149331   \n",
      "27            34  A002  R051  02-00-00  05-05-11  20:00:00  REGULAR   3150639   \n",
      "28            35  A002  R051  02-00-00  05-06-11  00:00:00  REGULAR   3150815   \n",
      "29            36  A002  R051  02-00-00  05-06-11  04:00:00  REGULAR   3150840   \n",
      "...          ...   ...   ...       ...       ...       ...      ...       ...   \n",
      "4202        4956  A034  R170  03-06-00  05-01-11  09:00:00  REGULAR   9164155   \n",
      "4203        4957  A034  R170  03-06-00  05-01-11  13:00:00  REGULAR   9164311   \n",
      "4204        4958  A034  R170  03-06-00  05-01-11  17:00:00  REGULAR   9164569   \n",
      "4205        4959  A034  R170  03-06-00  05-01-11  21:00:00  REGULAR   9164769   \n",
      "4206        4960  A034  R170  03-06-00  05-02-11  01:00:00  REGULAR   9164854   \n",
      "4207        4961  A034  R170  03-06-00  05-02-11  05:00:00  REGULAR   9164859   \n",
      "4208        4962  A034  R170  03-06-00  05-02-11  09:00:00  REGULAR   9164941   \n",
      "4209        4963  A034  R170  03-06-00  05-02-11  13:00:00  REGULAR   9165124   \n",
      "4210        4964  A034  R170  03-06-00  05-02-11  17:00:00  REGULAR   9165461   \n",
      "4211        4965  A034  R170  03-06-00  05-02-11  21:00:00  REGULAR   9165857   \n",
      "4212        4966  A034  R170  03-06-00  05-03-11  01:00:00  REGULAR   9165949   \n",
      "4213        4967  A034  R170  03-06-00  05-03-11  05:00:00  REGULAR   9165956   \n",
      "4214        4968  A034  R170  03-06-00  05-03-11  09:00:00  REGULAR   9166043   \n",
      "4215        4974  A034  R170  03-06-00  05-03-11  21:00:00  REGULAR   9167110   \n",
      "4216        4975  A034  R170  03-06-00  05-04-11  01:00:00  REGULAR   9167276   \n",
      "4217        4976  A034  R170  03-06-00  05-04-11  05:00:00  REGULAR   9167293   \n",
      "4218        4977  A034  R170  03-06-00  05-04-11  09:00:00  REGULAR   9167375   \n",
      "4219        4978  A034  R170  03-06-00  05-04-11  13:00:00  REGULAR   9167531   \n",
      "4220        4979  A034  R170  03-06-00  05-04-11  17:00:00  REGULAR   9167797   \n",
      "4221        4980  A034  R170  03-06-00  05-04-11  21:00:00  REGULAR   9168228   \n",
      "4222        4981  A034  R170  03-06-00  05-05-11  01:00:00  REGULAR   9168382   \n",
      "4223        4982  A034  R170  03-06-00  05-05-11  05:00:00  REGULAR   9168395   \n",
      "4224        4983  A034  R170  03-06-00  05-05-11  09:00:00  REGULAR   9168483   \n",
      "4225        4984  A034  R170  03-06-00  05-05-11  13:00:00  REGULAR   9168683   \n",
      "4226        4985  A034  R170  03-06-00  05-05-11  17:00:00  REGULAR   9169064   \n",
      "4227        4986  A034  R170  03-06-00  05-05-11  21:00:00  REGULAR   9169547   \n",
      "4228        4991  A034  R170  03-06-00  05-06-11  05:00:00  REGULAR   9169787   \n",
      "4229        4995  A034  R170  03-06-00  05-06-11  21:00:00  REGULAR   9171025   \n",
      "4230        4996  A034  R170  03-06-01  05-01-11  01:00:00  REGULAR   5604407   \n",
      "4231        4997  A034  R170  03-06-01  05-01-11  05:00:00  REGULAR   5604511   \n",
      "\n",
      "       EXITSn  ENTRIESn_hourly  EXITSn_hourly  \n",
      "0     1088151                0              0  \n",
      "1     1088159               23              8  \n",
      "2     1088177               18             18  \n",
      "3     1088231               71             54  \n",
      "4     1088275              170             44  \n",
      "5     1088317              214             42  \n",
      "6     1088328               87             11  \n",
      "7     1088331               10              3  \n",
      "8     1088420               36             89  \n",
      "9     1088753              153            333  \n",
      "10    1088823              243             70  \n",
      "11    1088888              831             65  \n",
      "12    1088918              154             30  \n",
      "13    1088921               13              3  \n",
      "14    1089014               36             93  \n",
      "15    1089341              139            327  \n",
      "16    1089417              280             76  \n",
      "17    1089478              825             61  \n",
      "18    1089515              183             37  \n",
      "19    1089520               11              5  \n",
      "20    1089632               50            112  \n",
      "21    1089965              140            333  \n",
      "22    1090054              277             89  \n",
      "23    1090117              832             63  \n",
      "24    1090139              173             22  \n",
      "25    1090145               16              6  \n",
      "26    1090257               34            112  \n",
      "27    1090714             1308            457  \n",
      "28    1090750              176             36  \n",
      "29    1090759               25              9  \n",
      "...       ...              ...            ...  \n",
      "4202  5796594               17              3  \n",
      "4203  5796677              156             83  \n",
      "4204  5796765              258             88  \n",
      "4205  5796838              200             73  \n",
      "4206  5796848               85             10  \n",
      "4207  5796849                5              1  \n",
      "4208  5796900               82             51  \n",
      "4209  5796959              183             59  \n",
      "4210  5797136              337            177  \n",
      "4211  5797317              396            181  \n",
      "4212  5797327               92             10  \n",
      "4213  5797327                7              0  \n",
      "4214  5797368               87             41  \n",
      "4215  5797892             1067            524  \n",
      "4216  5797906              166             14  \n",
      "4217  5797907               17              1  \n",
      "4218  5797946               82             39  \n",
      "4219  5798020              156             74  \n",
      "4220  5798161              266            141  \n",
      "4221  5798360              431            199  \n",
      "4222  5798378              154             18  \n",
      "4223  5798380               13              2  \n",
      "4224  5798410               88             30  \n",
      "4225  5798510              200            100  \n",
      "4226  5798683              381            173  \n",
      "4227  5798956              483            273  \n",
      "4228  5799002              240             46  \n",
      "4229  5799732             1238            730  \n",
      "4230  2448624                0              0  \n",
      "4231  2448644              104             20  \n",
      "\n",
      "[4232 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "def get_hourly_entries(df):\n",
    "    '''\n",
    "    The data in the MTA Subway Turnstile data reports on the cumulative\n",
    "    number of entries and exits per row.  Assume that you have a dataframe\n",
    "    called df that contains only the rows for a particular turnstile machine\n",
    "    (i.e., unique SCP, C/A, and UNIT).  This function should change\n",
    "    these cumulative entry numbers to a count of entries since the last reading\n",
    "    (i.e., entries since the last row in the dataframe).\n",
    "    \n",
    "    More specifically, you want to do two things:\n",
    "       1) Create a new column called ENTRIESn_hourly\n",
    "       2) Assign to the column the difference between ENTRIESn of the current row \n",
    "          and the previous row. If there is any NaN, fill/replace it with 1.\n",
    "    \n",
    "    You may find the pandas functions shift() and fillna() to be helpful in this exercise.\n",
    "    \n",
    "    Examples of what your dataframe should look like at the end of this exercise:\n",
    "    \n",
    "           C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn    EXITSn  ENTRIESn_hourly\n",
    "    0     A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   1088151                1\n",
    "    1     A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   1088159               23\n",
    "    2     A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   1088177               18\n",
    "    3     A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   1088231               71\n",
    "    4     A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   1088275              170\n",
    "    5     A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   1088317              214\n",
    "    6     A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   1088328               87\n",
    "    7     A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   1088331               10\n",
    "    8     A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   1088420               36\n",
    "    9     A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   1088753              153\n",
    "    10    A002  R051  02-00-00  05-02-11  16:00:00  REGULAR   3145337   1088823              243\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "    '''\n",
    "    #your code here\n",
    "    df0=df['ENTRIESn']\n",
    "    #print df0\n",
    "    df1=df['ENTRIESn'].shift(1)\n",
    "    df3=df0-df1\n",
    "    #print df3\n",
    "    p=pandas.Series(df3, index=df.index)\n",
    "    df['ENTRIESn_hourly']=p.fillna(1)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_subset_get_hours_entries.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_df = turnstile_master.groupby(['C/A','UNIT','SCP']).apply(get_hourly_exits)\n",
    "    print student_df\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0   C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn  \\\n",
      "0              0  A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   \n",
      "1              1  A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   \n",
      "2              2  A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   \n",
      "3              3  A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   \n",
      "4              4  A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   \n",
      "5              5  A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   \n",
      "6              6  A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   \n",
      "7              7  A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   \n",
      "8              8  A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   \n",
      "9              9  A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   \n",
      "10            10  A002  R051  02-00-00  05-02-11  16:00:00  REGULAR   3145337   \n",
      "11            11  A002  R051  02-00-00  05-02-11  20:00:00  REGULAR   3146168   \n",
      "12            12  A002  R051  02-00-00  05-03-11  00:00:00  REGULAR   3146322   \n",
      "13            13  A002  R051  02-00-00  05-03-11  04:00:00  REGULAR   3146335   \n",
      "14            14  A002  R051  02-00-00  05-03-11  08:00:00  REGULAR   3146371   \n",
      "15            15  A002  R051  02-00-00  05-03-11  12:00:00  REGULAR   3146510   \n",
      "16            16  A002  R051  02-00-00  05-03-11  16:00:00  REGULAR   3146790   \n",
      "17            17  A002  R051  02-00-00  05-03-11  20:00:00  REGULAR   3147615   \n",
      "18            18  A002  R051  02-00-00  05-04-11  00:00:00  REGULAR   3147798   \n",
      "19            19  A002  R051  02-00-00  05-04-11  04:00:00  REGULAR   3147809   \n",
      "20            20  A002  R051  02-00-00  05-04-11  08:00:00  REGULAR   3147859   \n",
      "21            21  A002  R051  02-00-00  05-04-11  12:00:00  REGULAR   3147999   \n",
      "22            22  A002  R051  02-00-00  05-04-11  16:00:00  REGULAR   3148276   \n",
      "23            23  A002  R051  02-00-00  05-04-11  20:00:00  REGULAR   3149108   \n",
      "24            24  A002  R051  02-00-00  05-05-11  00:00:00  REGULAR   3149281   \n",
      "25            25  A002  R051  02-00-00  05-05-11  04:00:00  REGULAR   3149297   \n",
      "26            26  A002  R051  02-00-00  05-05-11  08:00:00  REGULAR   3149331   \n",
      "27            34  A002  R051  02-00-00  05-05-11  20:00:00  REGULAR   3150639   \n",
      "28            35  A002  R051  02-00-00  05-06-11  00:00:00  REGULAR   3150815   \n",
      "29            36  A002  R051  02-00-00  05-06-11  04:00:00  REGULAR   3150840   \n",
      "...          ...   ...   ...       ...       ...       ...      ...       ...   \n",
      "4202        4956  A034  R170  03-06-00  05-01-11  09:00:00  REGULAR   9164155   \n",
      "4203        4957  A034  R170  03-06-00  05-01-11  13:00:00  REGULAR   9164311   \n",
      "4204        4958  A034  R170  03-06-00  05-01-11  17:00:00  REGULAR   9164569   \n",
      "4205        4959  A034  R170  03-06-00  05-01-11  21:00:00  REGULAR   9164769   \n",
      "4206        4960  A034  R170  03-06-00  05-02-11  01:00:00  REGULAR   9164854   \n",
      "4207        4961  A034  R170  03-06-00  05-02-11  05:00:00  REGULAR   9164859   \n",
      "4208        4962  A034  R170  03-06-00  05-02-11  09:00:00  REGULAR   9164941   \n",
      "4209        4963  A034  R170  03-06-00  05-02-11  13:00:00  REGULAR   9165124   \n",
      "4210        4964  A034  R170  03-06-00  05-02-11  17:00:00  REGULAR   9165461   \n",
      "4211        4965  A034  R170  03-06-00  05-02-11  21:00:00  REGULAR   9165857   \n",
      "4212        4966  A034  R170  03-06-00  05-03-11  01:00:00  REGULAR   9165949   \n",
      "4213        4967  A034  R170  03-06-00  05-03-11  05:00:00  REGULAR   9165956   \n",
      "4214        4968  A034  R170  03-06-00  05-03-11  09:00:00  REGULAR   9166043   \n",
      "4215        4974  A034  R170  03-06-00  05-03-11  21:00:00  REGULAR   9167110   \n",
      "4216        4975  A034  R170  03-06-00  05-04-11  01:00:00  REGULAR   9167276   \n",
      "4217        4976  A034  R170  03-06-00  05-04-11  05:00:00  REGULAR   9167293   \n",
      "4218        4977  A034  R170  03-06-00  05-04-11  09:00:00  REGULAR   9167375   \n",
      "4219        4978  A034  R170  03-06-00  05-04-11  13:00:00  REGULAR   9167531   \n",
      "4220        4979  A034  R170  03-06-00  05-04-11  17:00:00  REGULAR   9167797   \n",
      "4221        4980  A034  R170  03-06-00  05-04-11  21:00:00  REGULAR   9168228   \n",
      "4222        4981  A034  R170  03-06-00  05-05-11  01:00:00  REGULAR   9168382   \n",
      "4223        4982  A034  R170  03-06-00  05-05-11  05:00:00  REGULAR   9168395   \n",
      "4224        4983  A034  R170  03-06-00  05-05-11  09:00:00  REGULAR   9168483   \n",
      "4225        4984  A034  R170  03-06-00  05-05-11  13:00:00  REGULAR   9168683   \n",
      "4226        4985  A034  R170  03-06-00  05-05-11  17:00:00  REGULAR   9169064   \n",
      "4227        4986  A034  R170  03-06-00  05-05-11  21:00:00  REGULAR   9169547   \n",
      "4228        4991  A034  R170  03-06-00  05-06-11  05:00:00  REGULAR   9169787   \n",
      "4229        4995  A034  R170  03-06-00  05-06-11  21:00:00  REGULAR   9171025   \n",
      "4230        4996  A034  R170  03-06-01  05-01-11  01:00:00  REGULAR   5604407   \n",
      "4231        4997  A034  R170  03-06-01  05-01-11  05:00:00  REGULAR   5604511   \n",
      "\n",
      "       EXITSn  ENTRIESn_hourly  EXITSn_hourly  \n",
      "0     1088151                0              0  \n",
      "1     1088159               23              8  \n",
      "2     1088177               18             18  \n",
      "3     1088231               71             54  \n",
      "4     1088275              170             44  \n",
      "5     1088317              214             42  \n",
      "6     1088328               87             11  \n",
      "7     1088331               10              3  \n",
      "8     1088420               36             89  \n",
      "9     1088753              153            333  \n",
      "10    1088823              243             70  \n",
      "11    1088888              831             65  \n",
      "12    1088918              154             30  \n",
      "13    1088921               13              3  \n",
      "14    1089014               36             93  \n",
      "15    1089341              139            327  \n",
      "16    1089417              280             76  \n",
      "17    1089478              825             61  \n",
      "18    1089515              183             37  \n",
      "19    1089520               11              5  \n",
      "20    1089632               50            112  \n",
      "21    1089965              140            333  \n",
      "22    1090054              277             89  \n",
      "23    1090117              832             63  \n",
      "24    1090139              173             22  \n",
      "25    1090145               16              6  \n",
      "26    1090257               34            112  \n",
      "27    1090714             1308            457  \n",
      "28    1090750              176             36  \n",
      "29    1090759               25              9  \n",
      "...       ...              ...            ...  \n",
      "4202  5796594               17              3  \n",
      "4203  5796677              156             83  \n",
      "4204  5796765              258             88  \n",
      "4205  5796838              200             73  \n",
      "4206  5796848               85             10  \n",
      "4207  5796849                5              1  \n",
      "4208  5796900               82             51  \n",
      "4209  5796959              183             59  \n",
      "4210  5797136              337            177  \n",
      "4211  5797317              396            181  \n",
      "4212  5797327               92             10  \n",
      "4213  5797327                7              0  \n",
      "4214  5797368               87             41  \n",
      "4215  5797892             1067            524  \n",
      "4216  5797906              166             14  \n",
      "4217  5797907               17              1  \n",
      "4218  5797946               82             39  \n",
      "4219  5798020              156             74  \n",
      "4220  5798161              266            141  \n",
      "4221  5798360              431            199  \n",
      "4222  5798378              154             18  \n",
      "4223  5798380               13              2  \n",
      "4224  5798410               88             30  \n",
      "4225  5798510              200            100  \n",
      "4226  5798683              381            173  \n",
      "4227  5798956              483            273  \n",
      "4228  5799002              240             46  \n",
      "4229  5799732             1238            730  \n",
      "4230  2448624                0              0  \n",
      "4231  2448644              104             20  \n",
      "\n",
      "[4232 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "def get_hourly_exits(df):\n",
    "    '''\n",
    "    The data in the MTA Subway Turnstile data reports on the cumulative\n",
    "    number of entries and exits per row.  Assume that you have a dataframe\n",
    "    called df that contains only the rows for a particular turnstile machine\n",
    "    (i.e., unique SCP, C/A, and UNIT).  This function should change\n",
    "    these cumulative exit numbers to a count of exits since the last reading\n",
    "    (i.e., exits since the last row in the dataframe).\n",
    "    \n",
    "    More specifically, you want to do two things:\n",
    "       1) Create a new column called EXITSn_hourly\n",
    "       2) Assign to the column the difference between EXITSn of the current row \n",
    "          and the previous row. If there is any NaN, fill/replace it with 0.\n",
    "    \n",
    "    You may find the pandas functions shift() and fillna() to be helpful in this exercise.\n",
    "    \n",
    "    Example dataframe below:\n",
    "\n",
    "          Unnamed: 0   C/A  UNIT       SCP     DATEn     TIMEn    DESCn  ENTRIESn    EXITSn  ENTRIESn_hourly  EXITSn_hourly\n",
    "    0              0  A002  R051  02-00-00  05-01-11  00:00:00  REGULAR   3144312   1088151                0              0\n",
    "    1              1  A002  R051  02-00-00  05-01-11  04:00:00  REGULAR   3144335   1088159               23              8\n",
    "    2              2  A002  R051  02-00-00  05-01-11  08:00:00  REGULAR   3144353   1088177               18             18\n",
    "    3              3  A002  R051  02-00-00  05-01-11  12:00:00  REGULAR   3144424   1088231               71             54\n",
    "    4              4  A002  R051  02-00-00  05-01-11  16:00:00  REGULAR   3144594   1088275              170             44\n",
    "    5              5  A002  R051  02-00-00  05-01-11  20:00:00  REGULAR   3144808   1088317              214             42\n",
    "    6              6  A002  R051  02-00-00  05-02-11  00:00:00  REGULAR   3144895   1088328               87             11\n",
    "    7              7  A002  R051  02-00-00  05-02-11  04:00:00  REGULAR   3144905   1088331               10              3\n",
    "    8              8  A002  R051  02-00-00  05-02-11  08:00:00  REGULAR   3144941   1088420               36             89\n",
    "    9              9  A002  R051  02-00-00  05-02-11  12:00:00  REGULAR   3145094   1088753              153            333\n",
    "    '''\n",
    "    \n",
    "    #your code here\n",
    "    df0=df['EXITSn']\n",
    "    df1=df['EXITSn'].shift(1)\n",
    "    df2=df0-df1\n",
    "    p=pandas.Series(df2, index=df.index)\n",
    "    df['EXITSn_hourly']=p.fillna(0)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_subset_get_hours_entries.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_df = turnstile_master.groupby(['C/A','UNIT','SCP']).apply(get_hourly_exits)\n",
    "    print student_df\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       4\n",
      "2       8\n",
      "3      12\n",
      "4      16\n",
      "5      20\n",
      "6       0\n",
      "7       4\n",
      "8       8\n",
      "9      12\n",
      "10     16\n",
      "11     20\n",
      "12      0\n",
      "13      4\n",
      "14      8\n",
      "15     12\n",
      "16     16\n",
      "17     20\n",
      "18      0\n",
      "19      4\n",
      "20      8\n",
      "21     12\n",
      "22     16\n",
      "23     20\n",
      "24      0\n",
      "25      4\n",
      "26      8\n",
      "27     12\n",
      "28     16\n",
      "29     20\n",
      "       ..\n",
      "346     5\n",
      "347     9\n",
      "348    13\n",
      "349    17\n",
      "350    21\n",
      "351     1\n",
      "352     5\n",
      "353     9\n",
      "354    13\n",
      "355    17\n",
      "356    21\n",
      "357     1\n",
      "358     5\n",
      "359     9\n",
      "360    13\n",
      "361    17\n",
      "362    21\n",
      "363     1\n",
      "364     5\n",
      "365     9\n",
      "366    13\n",
      "367    17\n",
      "368    21\n",
      "369     1\n",
      "370     5\n",
      "371     9\n",
      "372    13\n",
      "373    16\n",
      "374    17\n",
      "375    21\n",
      "Name: Hour, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "def time_to_hour(time):\n",
    "    '''\n",
    "    Given an input variable time that represents time in the format of:\n",
    "    \"00:00:00\" (hour:minutes:seconds)\n",
    "    \n",
    "    Write a function to extract the hour part from the input variable time\n",
    "    and return it as an integer. For example:\n",
    "        1) if hour is 00, your code should return 0\n",
    "        2) if hour is 01, your code should return 1\n",
    "        3) if hour is 21, your code should return 21\n",
    "        \n",
    "    Please return hour as an integer.\n",
    "    '''\n",
    "    \n",
    "    hour = (ord(time[0])-48)*10+ord(time[1])-48\n",
    "    return hour\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_subset_consolidate_rows.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_df = turnstile_master.copy(deep=True)\n",
    "    student_df['Hour'] = student_df['TIMEn'].map(time_to_hour)\n",
    "    print student_df['Hour']\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2011-05-01\n",
      "1      2011-05-01\n",
      "2      2011-05-01\n",
      "3      2011-05-01\n",
      "4      2011-05-01\n",
      "5      2011-05-01\n",
      "6      2011-05-02\n",
      "7      2011-05-02\n",
      "8      2011-05-02\n",
      "9      2011-05-02\n",
      "10     2011-05-02\n",
      "11     2011-05-02\n",
      "12     2011-05-03\n",
      "13     2011-05-03\n",
      "14     2011-05-03\n",
      "15     2011-05-03\n",
      "16     2011-05-03\n",
      "17     2011-05-03\n",
      "18     2011-05-04\n",
      "19     2011-05-04\n",
      "20     2011-05-04\n",
      "21     2011-05-04\n",
      "22     2011-05-04\n",
      "23     2011-05-04\n",
      "24     2011-05-05\n",
      "25     2011-05-05\n",
      "26     2011-05-05\n",
      "27     2011-05-05\n",
      "28     2011-05-05\n",
      "29     2011-05-05\n",
      "          ...    \n",
      "346    2011-05-02\n",
      "347    2011-05-02\n",
      "348    2011-05-02\n",
      "349    2011-05-02\n",
      "350    2011-05-02\n",
      "351    2011-05-03\n",
      "352    2011-05-03\n",
      "353    2011-05-03\n",
      "354    2011-05-03\n",
      "355    2011-05-03\n",
      "356    2011-05-03\n",
      "357    2011-05-04\n",
      "358    2011-05-04\n",
      "359    2011-05-04\n",
      "360    2011-05-04\n",
      "361    2011-05-04\n",
      "362    2011-05-04\n",
      "363    2011-05-05\n",
      "364    2011-05-05\n",
      "365    2011-05-05\n",
      "366    2011-05-05\n",
      "367    2011-05-05\n",
      "368    2011-05-05\n",
      "369    2011-05-06\n",
      "370    2011-05-06\n",
      "371    2011-05-06\n",
      "372    2011-05-06\n",
      "373    2011-05-06\n",
      "374    2011-05-06\n",
      "375    2011-05-06\n",
      "Name: DATEn, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def reformat_subway_dates(date):\n",
    "    '''\n",
    "    The dates in our subway data are formatted in the format month-day-year.\n",
    "    The dates in our weather underground data are formatted year-month-day.\n",
    "    \n",
    "    In order to join these two data sets together, we'll want the dates formatted\n",
    "    the same way.  Write a function that takes as its input a date in the MTA Subway\n",
    "    data format, and returns a date in the weather underground format.\n",
    "    \n",
    "    Hint: \n",
    "    There are a couple of useful functions in the datetime library that will\n",
    "    help on this assignment, called strptime and strftime. \n",
    "    More info can be seen here and further in the documentation section:\n",
    "    http://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\n",
    "    '''\n",
    "\n",
    "    date_formatted = datetime.datetime.strptime(date,\"%m-%d-%y\").strftime(\"%Y-%m-%d\")\n",
    "    return date_formatted\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_subset_time_to_hours.csv\"\n",
    "    #output_filename = \"output.csv\"\n",
    "\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_df = turnstile_master.copy(deep=True)\n",
    "    student_df['DATEn'] = student_df['DATEn'].map(reformat_subway_dates)\n",
    "    print student_df['DATEn']\n",
    "    #student_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, Ttest_indResult(statistic=3.9867064465971302, pvalue=7.482391590970722e-05))\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy.stats\n",
    "import pandas\n",
    "\n",
    "def compare_averages(filename):\n",
    "    \"\"\"\n",
    "    Performs a t-test on two sets of baseball data (left-handed and right-handed hitters).\n",
    "\n",
    "    You will be given a csv file that has three columns.  A player's\n",
    "    name, handedness (L for lefthanded or R for righthanded) and their\n",
    "    career batting average (called 'avg'). You can look at the csv\n",
    "    file by downloading the baseball_stats file from Downloadables below. \n",
    "    \n",
    "    Write a function that will read that the csv file into a pandas data frame,\n",
    "    and run Welch's t-test on the two cohorts defined by handedness.\n",
    "    \n",
    "    One cohort should be a data frame of right-handed batters. And the other\n",
    "    cohort should be a data frame of left-handed batters.\n",
    "    \n",
    "    We have included the scipy.stats library to help you write\n",
    "    or implement Welch's t-test:\n",
    "    http://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "    \n",
    "    With a significance level of 95%, if there is no difference\n",
    "    between the two cohorts, return a tuple consisting of\n",
    "    True, and then the tuple returned by scipy.stats.ttest.  \n",
    "    \n",
    "    If there is a difference, return a tuple consisting of\n",
    "    False, and then the tuple returned by scipy.stats.ttest.\n",
    "    \n",
    "    For example, the tuple that you return may look like:\n",
    "    (True, (9.93570222, 0.000023))\n",
    "    \"\"\"\n",
    "    baseball_data=pandas.read_csv(filename)\n",
    "    ldata=baseball_data[baseball_data[\"handedness\"]=='L']\n",
    "    rdata=baseball_data[baseball_data[\"handedness\"]=='R']\n",
    "\n",
    "    rg = scipy.stats.ttest_ind(ldata['avg'],rdata['avg'], equal_var=False)\n",
    "    if rg[1]>=0.5:\n",
    "        return(True,rg)\n",
    "    else:\n",
    "        return(False,rg)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filename = \"baseball_data.csv\"\n",
    "    result = compare_averages(filename)\n",
    "    print result\n",
    "    \n",
    "#The correct t-statistic is +/-9.93570222624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entries_histogram(turnstile_weather):\n",
    "    '''\n",
    "    Before we perform any analysis, it might be useful to take a\n",
    "    look at the data we're hoping to analyze. More specifically, let's \n",
    "    examine the hourly entries in our NYC subway data and determine what\n",
    "    distribution the data follows. This data is stored in a dataframe\n",
    "    called turnstile_weather under the ['ENTRIESn_hourly'] column.\n",
    "    \n",
    "    Let's plot two histograms on the same axes to show hourly\n",
    "    entries when raining vs. when not raining. Here's an example on how\n",
    "    to plot histograms with pandas and matplotlib:\n",
    "    turnstile_weather['column_to_graph'].hist()\n",
    "    \n",
    "    Your histogram may look similar to bar graph in the instructor notes below.\n",
    "    \n",
    "    You can read a bit about using matplotlib and pandas to plot histograms here:\n",
    "    http://pandas.pydata.org/pandas-docs/stable/visualization.html#histograms\n",
    "    \n",
    "    You can see the information contained within the turnstile weather data here:\n",
    "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv\n",
    "    '''\n",
    "    \n",
    "    plt.figure()\n",
    "    turnstile_weather[turnstile_weather['rain']==1]['ENTRIESn_hourly'].hist() # your code here to plot a historgram for hourly entries when it is raining\n",
    "    turnstile_weather[turnstile_weather['rain']==0]['ENTRIESn_hourly'].hist() # your code here to plot a historgram for hourly entries when it is not raining\n",
    "    return plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = \"plot.png\"\n",
    "    turnstile_weather = pd.read_csv(\"turnstile_data_master_with_weather.csv\")\n",
    "    plt = entries_histogram(turnstile_weather)\n",
    "    plt.savefig(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1105.4463767458733, 1090.278780151855, 1924409167.0, 0.024999912793489721)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import pandas\n",
    "\n",
    "def mann_whitney_plus_means(turnstile_weather):\n",
    "    '''\n",
    "    This function will consume the turnstile_weather dataframe containing\n",
    "    our final turnstile weather data. \n",
    "    \n",
    "    You will want to take the means and run the Mann Whitney U-test on the \n",
    "    ENTRIESn_hourly column in the turnstile_weather dataframe.\n",
    "    \n",
    "    This function should return:\n",
    "        1) the mean of entries with rain\n",
    "        2) the mean of entries without rain\n",
    "        3) the Mann-Whitney U-statistic and p-value comparing the number of entries\n",
    "           with rain and the number of entries without rain\n",
    "    \n",
    "    You should feel free to use scipy's Mann-Whitney implementation, and you \n",
    "    might also find it useful to use numpy's mean function.\n",
    "    \n",
    "    Here are the functions' documentation:\n",
    "    http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "    http://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\n",
    "    \n",
    "    You can look at the final turnstile weather data at the link below:\n",
    "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv\n",
    "    '''\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    wrm=turnstile_weather[turnstile_weather['rain']==1]['ENTRIESn_hourly']\n",
    "    with_rain_mean=np.mean(wrm)\n",
    "    worm=turnstile_weather[turnstile_weather['rain']==0]['ENTRIESn_hourly']\n",
    "    without_rain_mean=np.mean(worm)\n",
    "    [U,p]=scipy.stats.mannwhitneyu(wrm, worm)\n",
    "    \n",
    "    return with_rain_mean, without_rain_mean, U, p # leave this line for the grader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_with_weather.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    student_output = mann_whitney_plus_means(turnstile_master)\n",
    "\n",
    "    print student_output\n",
    "    \n",
    "#Here's the correct output:\n",
    "#(1105.4463767458733, 1090.278780151855, 1924409167.0, 0.024999912793489721)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta =\n",
      "[ 45.35759233  -9.02442042  13.69229668]\n",
      "\n",
      "Cost History = \n",
      "0      3748.133469\n",
      "1      3727.492258\n",
      "2      3707.261946\n",
      "3      3687.434249\n",
      "4      3668.001052\n",
      "5      3648.954405\n",
      "6      3630.286519\n",
      "7      3611.989767\n",
      "8      3594.056675\n",
      "9      3576.479921\n",
      "10     3559.252334\n",
      "11     3542.366888\n",
      "12     3525.816700\n",
      "13     3509.595027\n",
      "14     3493.695263\n",
      "15     3478.110938\n",
      "16     3462.835711\n",
      "17     3447.863371\n",
      "18     3433.187834\n",
      "19     3418.803138\n",
      "20     3404.703444\n",
      "21     3390.883030\n",
      "22     3377.336290\n",
      "23     3364.057733\n",
      "24     3351.041978\n",
      "25     3338.283754\n",
      "26     3325.777897\n",
      "27     3313.519347\n",
      "28     3301.503147\n",
      "29     3289.724439\n",
      "          ...     \n",
      "970    2686.739192\n",
      "971    2686.738609\n",
      "972    2686.738029\n",
      "973    2686.737453\n",
      "974    2686.736881\n",
      "975    2686.736312\n",
      "976    2686.735747\n",
      "977    2686.735186\n",
      "978    2686.734628\n",
      "979    2686.734074\n",
      "980    2686.733523\n",
      "981    2686.732975\n",
      "982    2686.732431\n",
      "983    2686.731891\n",
      "984    2686.731354\n",
      "985    2686.730820\n",
      "986    2686.730290\n",
      "987    2686.729764\n",
      "988    2686.729240\n",
      "989    2686.728720\n",
      "990    2686.728203\n",
      "991    2686.727690\n",
      "992    2686.727179\n",
      "993    2686.726672\n",
      "994    2686.726168\n",
      "995    2686.725668\n",
      "996    2686.725170\n",
      "997    2686.724676\n",
      "998    2686.724185\n",
      "999    2686.723697\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#############################################################################\n",
    "def normalize_features(array):\n",
    "   \"\"\"\n",
    "   Normalize the features in our data set.\n",
    "   \"\"\"\n",
    "   array_normalized = (array-array.mean())/array.std()\n",
    "   mu = array.mean()\n",
    "   sigma = array.std()\n",
    "\n",
    "   return array_normalized, mu, sigma\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function given a set of features / values, and the values for our thetas.\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predicted_values = np.dot(features, theta)\n",
    "        theta = theta - alpha / m * np.dot((predicted_values - values), features)\n",
    "\n",
    "        cost = compute_cost(features, values, theta)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return theta, pd.Series(cost_history)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Read data into a pandas dataframe.\n",
    "    data = pandas.read_csv('baseball_stats_regression.csv')\n",
    "\n",
    "    # Isolate features / values.\n",
    "    features = data[['height', 'weight']]\n",
    "    values = data[['HR']]\n",
    "    m = len(values)\n",
    "\n",
    "    # Normalize features.\n",
    "    features, mu, sigma = normalize_features(features)\n",
    "\n",
    "    # Add a column of ones to features for constant term.\n",
    "    features['ones'] = numpy.ones(m)\n",
    "    features_array = numpy.array(features[['ones', 'height', 'weight']])\n",
    "    values_array = numpy.array(values).flatten()\n",
    "\n",
    "    # Set values for alpha, number of iterations.\n",
    "    alpha = 0.01\n",
    "    num_iterations = 1000\n",
    "\n",
    "    # Initialize theta and perform gradient descent.\n",
    "    theta_gradient_descent = numpy.zeros(3)\n",
    "    theta_gradient_descent, cost_history = gradient_descent(features_array, values_array, theta_gradient_descent,\n",
    "                                                            alpha, num_iterations)\n",
    "\n",
    "    print \"Theta =\\n{theta}\\n\\nCost History = \\n{history}\".format(theta=theta_gradient_descent, history=cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:  1393.26788464\n",
      "params:  rain         -36.519468\n",
      "fog          103.125772\n",
      "Hour          67.390852\n",
      "meantempi     -8.492697\n",
      "dtype: float64\n",
      "0.458211888431\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression OLS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\"\"\"\n",
    "In this question, you need to:\n",
    "1) implement the linear_regression() procedure\n",
    "2) Select features (in the predictions procedure) and make predictions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def linear_regression(features, values):\n",
    "    \"\"\"\n",
    "    Perform linear regression given a data set with an arbitrary number of features.\n",
    "    \n",
    "    This can be the same code as in the lesson #3 exercise.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "    ###########################\n",
    "    features=sm.add_constant(features)\n",
    "    model=sm.OLS(values,features)\n",
    "    results=model.fit()\n",
    "    intercept = results.params[0]\n",
    "    params=results.params[1:]\n",
    "    \n",
    "    return intercept, params\n",
    "\n",
    "def predictions(dataframe):\n",
    "    '''\n",
    "    The NYC turnstile data is stored in a pandas dataframe called weather_turnstile.\n",
    "    Using the information stored in the dataframe, let's predict the ridership of\n",
    "    the NYC subway using linear regression with gradient descent.\n",
    "    \n",
    "    You can download the complete turnstile weather dataframe here:\n",
    "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv    \n",
    "    \n",
    "    Your prediction should have a R^2 value of 0.40 or better.\n",
    "    You need to experiment using various input features contained in the dataframe. \n",
    "    We recommend that you don't use the EXITSn_hourly feature as an input to the \n",
    "    linear model because we cannot use it as a predictor: we cannot use exits \n",
    "    counts as a way to predict entry counts. \n",
    "    '''\n",
    "    ################################ MODIFY THIS SECTION #####################################\n",
    "    # Select features. You should modify this section to try different features!             #\n",
    "    # We've selected rain, precipi, Hour, meantempi, and UNIT (as a dummy) to start you off. #\n",
    "    # See this page for more info about dummy variables:                                     #\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html          #\n",
    "    ##########################################################################################\n",
    "    features = dataframe[['rain', 'fog', 'Hour', 'meantempi']]\n",
    "    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "    features = features.join(dummy_units)\n",
    "    \n",
    "    # Values\n",
    "    values = dataframe['ENTRIESn_hourly']\n",
    "\n",
    "    # Perform linear regression\n",
    "    intercept, params = linear_regression(features, values)\n",
    "    predictions = intercept + np.dot(features, params)\n",
    "    \n",
    "    print \"intercept: \", intercept\n",
    "    print \"params: \", params[:4]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def compute_r_squared(data, predictions):\n",
    "    # Write a function that, given two input numpy arrays, 'data', and 'predictions,'\n",
    "    # returns the coefficient of determination, R^2, for the model that produced \n",
    "    # predictions.\n",
    "    # \n",
    "    # Numpy has a couple of functions -- np.mean() and np.sum() --\n",
    "    # that you might find useful, but you don't have to use them.\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    avg_data=np.mean(data)\n",
    "    r_squared = 1 - np.sum((data-predictions)**2)/np.sum((data-avg_data)**2)\n",
    "\n",
    "    return r_squared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_with_weather.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    predicted_values = predictions(turnstile_master)\n",
    "    r_squared = compute_r_squared(turnstile_master['ENTRIESn_hourly'], predicted_values) \n",
    "\n",
    "    print r_squared\n",
    "\n",
    "#Your r^2 value is 0.479832970696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402509348095\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression using SGDRegressor = 20\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\"\"\"\n",
    "In this question, you need to:\n",
    "1) Implement the linear_regression() procedure using gradient descent.\n",
    "   You can use the SGDRegressor class from sklearn, since this class uses gradient descent.\n",
    "2) Select features (in the predictions procedure) and make predictions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def normalize_features(features):\n",
    "    ''' \n",
    "    Returns the means and standard deviations of the given features, along with a normalized feature\n",
    "    matrix.\n",
    "    ''' \n",
    "    means = np.mean(features, axis=0)\n",
    "    std_devs = np.std(features, axis=0)\n",
    "    normalized_features = (features - means) / std_devs\n",
    "    return means, std_devs, normalized_features\n",
    "\n",
    "def recover_params(means, std_devs, norm_intercept, norm_params):\n",
    "    ''' \n",
    "    Recovers the weights for a linear model given parameters that were fitted using\n",
    "    normalized features. Takes the means and standard deviations of the original\n",
    "    features, along with the intercept and parameters computed using the normalized\n",
    "    features, and returns the intercept and parameters that correspond to the original\n",
    "    features.\n",
    "    ''' \n",
    "    intercept = norm_intercept - np.sum(means * norm_params / std_devs)\n",
    "    params = norm_params / std_devs\n",
    "    return intercept, params\n",
    "\n",
    "def linear_regression(features, values):\n",
    "    \"\"\"\n",
    "    Perform linear regression given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "    ###########################\n",
    "    clf = SGDRegressor(n_iter=20)\n",
    "    results=clf.fit(features,values)\n",
    "    params=clf.coef_\n",
    "    intercept=clf.intercept_\n",
    "    \n",
    "    return intercept, params\n",
    "\n",
    "def predictions(dataframe):\n",
    "    '''\n",
    "    The NYC turnstile data is stored in a pandas dataframe called weather_turnstile.\n",
    "    Using the information stored in the dataframe, let's predict the ridership of\n",
    "    the NYC subway using linear regression with gradient descent.\n",
    "    \n",
    "    You can download the complete turnstile weather dataframe here:\n",
    "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv    \n",
    "    \n",
    "    Your prediction should have a R^2 value of 0.40 or better.\n",
    "    You need to experiment using various input features contained in the dataframe. \n",
    "    We recommend that you don't use the EXITSn_hourly feature as an input to the \n",
    "    linear model because we cannot use it as a predictor: we cannot use exits \n",
    "    counts as a way to predict entry counts. \n",
    "    \n",
    "    Note: Due to the memory and CPU limitation of our Amazon EC2 instance, we will\n",
    "    give you a random subset (~50%) of the data contained in \n",
    "    turnstile_data_master_with_weather.csv. You are encouraged to experiment with \n",
    "    this exercise on your own computer, locally.\n",
    "    \n",
    "    If you receive a \"server has encountered an error\" message, that means you are \n",
    "    hitting the 30-second limit that's placed on running your program. Try using a\n",
    "    smaller number of features or fewer iterations.\n",
    "    '''\n",
    "    ################################ MODIFY THIS SECTION #####################################\n",
    "    # Select features. You should modify this section to try different features!             #\n",
    "    # We've selected rain, precipi, Hour, meantempi, and UNIT (as a dummy) to start you off. #\n",
    "    # See this page for more info about dummy variables:                                     #\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html          #\n",
    "    ##########################################################################################\n",
    "    features = dataframe[['rain', 'Hour', 'maxdewpti', 'precipi','meandewpti', 'meanwindspdi','meanpressurei','meantempi']]\n",
    "    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "    features = features.join(dummy_units)\n",
    "    \n",
    "    # Values\n",
    "    values = dataframe['ENTRIESn_hourly']\n",
    "    \n",
    "    # Get numpy arrays\n",
    "    features_array = features.values\n",
    "    values_array = values.values\n",
    "    \n",
    "    means, std_devs, normalized_features_array = normalize_features(features_array)\n",
    "\n",
    "    # Perform gradient descent\n",
    "    norm_intercept, norm_params = linear_regression(normalized_features_array, values_array)\n",
    "    intercept, params = recover_params(means, std_devs, norm_intercept, norm_params)\n",
    "    \n",
    "    print \"intercept: \", intercept\n",
    "    print \"params: \", params\n",
    "    \n",
    "    predictions = intercept + np.dot(features_array, params)\n",
    "    # The following line would be equivalent:\n",
    "    # predictions = norm_intercept + np.dot(normalized_features_array, norm_params)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_with_weather.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    predicted_values = predictions(turnstile_master)\n",
    "    r_squared = compute_r_squared(turnstile_master['ENTRIESn_hourly'], predicted_values) \n",
    "\n",
    "    print r_squared\n",
    "#Your r^2 value is 0.420636342409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458044314039\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression with Gradient Descent = 75\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#############################################################################\n",
    "def normalize_features(array):\n",
    "   \"\"\"\n",
    "   Normalize the features in our data set.\n",
    "   \"\"\"\n",
    "   array_normalized = (array-array.mean())/array.std()\n",
    "   mu = array.mean()\n",
    "   sigma = array.std()\n",
    "\n",
    "   return array_normalized, mu, sigma\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function given a set of features / values, and the values for our thetas.\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predicted_values = np.dot(features, theta)\n",
    "        theta = theta - alpha / m * np.dot((predicted_values - values), features)\n",
    "\n",
    "        cost = compute_cost(features, values, theta)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return theta, pd.Series(cost_history)\n",
    "\n",
    "def predictions(dataframe):\n",
    "\n",
    "    dummy_units = pd.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "    features = dataframe[['rain', 'precipi', 'Hour', 'meantempi']].join(dummy_units)\n",
    "    values = dataframe[['ENTRIESn_hourly']]\n",
    "    m = len(values)\n",
    "\n",
    "    features, mu, sigma = normalize_features(features)\n",
    "\n",
    "    features['ones'] = np.ones(m)\n",
    "    features_array = np.array(features)\n",
    "    values_array = np.array(values).flatten()\n",
    "\n",
    "    #Set values for alpha, number of iterations.\n",
    "    alpha = 0.1\n",
    "    num_iterations = 75\n",
    "\n",
    "    #Initialize theta, perform gradient descent\n",
    "    theta_gradient_descent = np.zeros(len(features.columns))\n",
    "    theta_gradient_descent, cost_history = gradient_descent(features_array, values_array, theta_gradient_descent,\n",
    "                                                            alpha, num_iterations)\n",
    "\n",
    "    predictions = np.dot(features_array, theta_gradient_descent)\n",
    "\n",
    "    return predictions\n",
    "#############################################################################\n",
    "def compute_r_squared(data, predictions):\n",
    "    # Write a function that, given two input numpy arrays, 'data', and 'predictions,'\n",
    "    # returns the coefficient of determination, R^2, for the model that produced \n",
    "    # predictions.\n",
    "    # \n",
    "    # Numpy has a couple of functions -- np.mean() and np.sum() --\n",
    "    # that you might find useful, but you don't have to use them.\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    avg_data=np.mean(data)\n",
    "    r_squared = 1 - np.sum((data-predictions)**2)/np.sum((data-avg_data)**2)\n",
    "\n",
    "    return r_squared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_with_weather.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    predictions = predictions(turnstile_master)\n",
    "    r_squared = compute_r_squared(turnstile_master['ENTRIESn_hourly'], predictions)\n",
    "    print r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot residuals for Gradient Descent = 75\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_residuals(turnstile_weather, predictions):\n",
    "    '''\n",
    "    Using the same methods that we used to plot a histogram of entries\n",
    "    per hour for our data, why don't you make a histogram of the residuals\n",
    "    (that is, the difference between the original hourly entry data and the predicted values).\n",
    "    Try different binwidths for your histogram.\n",
    "\n",
    "    Based on this residual histogram, do you have any insight into how our model\n",
    "    performed?  Reading a bit on this webpage might be useful:\n",
    "\n",
    "    http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm\n",
    "    '''\n",
    "    \n",
    "    plt.figure()\n",
    "    (turnstile_weather['ENTRIESn_hourly'] - predictions).hist()\n",
    "    return plt\n",
    "\n",
    "def normalize_features(array):\n",
    "   \"\"\"\n",
    "   Normalize the features in our data set.\n",
    "   \"\"\"\n",
    "   array_normalized = (array-array.mean())/array.std()\n",
    "   mu = array.mean()\n",
    "   sigma = array.std()\n",
    "\n",
    "   return array_normalized, mu, sigma\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function given a set of features / values, and the values for our thetas.\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predicted_values = np.dot(features, theta)\n",
    "        theta = theta - alpha / m * np.dot((predicted_values - values), features)\n",
    "\n",
    "        cost = compute_cost(features, values, theta)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return theta, pd.Series(cost_history)\n",
    "\n",
    "def predictions(dataframe):\n",
    "\n",
    "    dummy_units = pd.get_dummies(dataframe['UNIT'], prefix='unit')\n",
    "    features = dataframe[['rain', 'precipi', 'Hour', 'meantempi']].join(dummy_units)\n",
    "    values = dataframe[['ENTRIESn_hourly']]\n",
    "    m = len(values)\n",
    "\n",
    "    features, mu, sigma = normalize_features(features)\n",
    "\n",
    "    features['ones'] = np.ones(m)\n",
    "    features_array = np.array(features)\n",
    "    values_array = np.array(values).flatten()\n",
    "\n",
    "    #Set values for alpha, number of iterations.\n",
    "    alpha = 0.1\n",
    "    num_iterations = 75\n",
    "\n",
    "    #Initialize theta, perform gradient descent\n",
    "    theta_gradient_descent = np.zeros(len(features.columns))\n",
    "    theta_gradient_descent, cost_history = gradient_descent(features_array, values_array, theta_gradient_descent,\n",
    "                                                            alpha, num_iterations)\n",
    "    predictions = np.dot(features_array, theta_gradient_descent)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"turnstile_data_master_with_weather.csv\"\n",
    "    turnstile_master = pd.read_csv(input_filename)\n",
    "    prediction_values = predictions(turnstile_master)\n",
    "\n",
    "    image = \"plot.png\"\n",
    "    plt = plot_residuals(turnstile_master, prediction_values)\n",
    "    plt.savefig(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "from ggplot import *\n",
    "\n",
    "import pandas\n",
    "\n",
    "def lineplot(hr_year_csv):\n",
    "    # A csv file will be passed in as an argument which\n",
    "    # contains two columns -- 'HR' (the number of homerun hits)\n",
    "    # and 'yearID' (the year in which the homeruns were hit).\n",
    "    #\n",
    "    # Fill out the body of this function, lineplot, to use the\n",
    "    # passed-in csv file, hr_year.csv, and create a\n",
    "    # chart with points connected by lines, both colored 'red',\n",
    "    # showing the number of HR by year.\n",
    "    #\n",
    "    # You will want to first load the csv file into a pandas dataframe\n",
    "    # and use the pandas dataframe along with ggplot to create your visualization\n",
    "    #\n",
    "    # You can check out the data in the csv file at the link below:\n",
    "    # https://www.dropbox.com/s/awgdal71hc1u06d/hr_year.csv\n",
    "    #\n",
    "    # You can read more about ggplot at the following link:\n",
    "    # https://github.com/yhat/ggplot/\n",
    "    \n",
    "    df=pandas.read_csv(hr_year_csv)\n",
    "    gg = ggplot(df, aes(df['yearID'], df['HR']))+geom_point(color='red')+geom_line(color='red')\n",
    "    return gg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = \"hr_year.csv\"\n",
    "    image = \"plot.png\"\n",
    "    gg =  lineplot(data)\n",
    "    ggsave(image, gg, width=11, height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "\n",
    "def lineplot_compare(hr_by_team_year_sf_la_csv):\n",
    "    # Write a function, lineplot_compare, that will read a csv file\n",
    "    # called hr_by_team_year_sf_la.csv and plot it using pandas and ggplot.\n",
    "    #\n",
    "    # This csv file has three columns: yearID, HR, and teamID. The data in the\n",
    "    # file gives the total number of home runs hit each year by the SF Giants \n",
    "    # (teamID == 'SFN') and the LA Dodgers (teamID == \"LAN\"). Produce a \n",
    "    # visualization comparing the total home runs by year of the two teams. \n",
    "    # \n",
    "    # You can see the data in hr_by_team_year_sf_la_csv\n",
    "    # at the link below:\n",
    "    # https://www.dropbox.com/s/wn43cngo2wdle2b/hr_by_team_year_sf_la.csv\n",
    "    #\n",
    "    # Note that to differentiate between multiple categories on the \n",
    "    # same plot in ggplot, we can pass color in with the other arguments\n",
    "    # to aes, rather than in our geometry functions. For example, \n",
    "    # ggplot(data, aes(xvar, yvar, color=category_var)). This should help you \n",
    "    # in this exercise.\n",
    "    dd=pandas.read_csv(hr_by_team_year_sf_la_csv)\n",
    "    gg = ggplot(dd,aes('yearID','HR', color='teamID'))+geom_point()+geom_line()\n",
    "    return gg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = \"hr_by_team_year_sf_la.csv\"\n",
    "    image = \"plot.png\"\n",
    "    gg =  lineplot_compare(data)\n",
    "    ggsave(image, gg, width=11, height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "from ggplot import *\n",
    "import datetime\n",
    "\n",
    "def plot_weather_data(turnstile_weather):\n",
    "    ''' \n",
    "    plot_weather_data is passed a dataframe called turnstile_weather. \n",
    "    Use turnstile_weather along with ggplot to make another data visualization\n",
    "    focused on the MTA and weather data we used in Project 3.\n",
    "    \n",
    "    Make a type of visualization different than what you did in the previous exercise.\n",
    "    Try to use the data in a different way (e.g., if you made a lineplot concerning \n",
    "    ridership and time of day in exercise #1, maybe look at weather and try to make a \n",
    "    histogram in this exercise). Or try to use multiple encodings in your graph if \n",
    "    you didn't in the previous exercise.\n",
    "    \n",
    "    You should feel free to implement something that we discussed in class \n",
    "    (e.g., scatterplots, line plots, or histograms) or attempt to implement\n",
    "    something more advanced if you'd like.\n",
    "\n",
    "    Here are some suggestions for things to investigate and illustrate:\n",
    "     * Ridership by time-of-day or day-of-week\n",
    "     * How ridership varies by subway station (UNIT)\n",
    "     * Which stations have more exits or entries at different times of day\n",
    "       (You can use UNIT as a proxy for subway station.)\n",
    "\n",
    "    If you'd like to learn more about ggplot and its capabilities, take\n",
    "    a look at the documentation at:\n",
    "    https://pypi.python.org/pypi/ggplot/\n",
    "     \n",
    "    You can check out the link \n",
    "    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv\n",
    "    to see all the columns and data points included in the turnstile_weather \n",
    "    dataframe.\n",
    "     \n",
    "   However, due to the limitation of our Amazon EC2 server, we are giving you a random\n",
    "    subset, about 1/3 of the actual data in the turnstile_weather dataframe.\n",
    "    '''\n",
    "    dataTW = turnstile_weather\n",
    "    entries_DayOfMonth = dataTW[['DATEn', 'ENTRIESn_hourly']].groupby('DATEn', as_index=False).sum()\n",
    "    entries_DayOfMonth['Day'] = [datetime.strptime(x, '%Y-%m-%d').strftime('%w %A') \n",
    "    for x in entries_DayOfMonth['DATEn']]\n",
    "    entries_Day = entries_DayOfMonth[['Day', 'ENTRIESn_hourly']].groupby('Day', as_index=False).sum()\n",
    "    plot = ggplot(entries_Day, aes(x='Day', y='ENTRIESn_hourly')) \\\n",
    "    + geom_bar(aes(weight='ENTRIESn_hourly'), fill='red',stat=\"bar\") \\\n",
    "           + ggtitle('NYC Subway ridership / day of week') \\\n",
    "        + xlab('Day') + ylab('Entries')\n",
    "    return plot\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = \"plot.png\"\n",
    "    with open(image, \"wb\") as f:\n",
    "        turnstile_weather = pandas.read_csv(input_filename)\n",
    "        turnstile_weather['datetime'] = turnstile_weather['DATEn'] + ' ' + turnstile_weather['TIMEn']\n",
    "        gg =  plot_weather_data(turnstile_weather)\n",
    "        ggsave(f, gg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
